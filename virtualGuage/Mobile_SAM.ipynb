{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l91g4z_znTEc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "from numpy import zeros\n",
        "from numpy.random import randint\n",
        "import torch\n",
        "import os\n",
        "import cv2\n",
        "from statistics import mean\n",
        "from torch.nn.functional import threshold, normalize\n",
        "\n",
        "# Data Viz\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torch torchvision &> /dev/null\n",
        "! pip install opencv-python pycocotools matplotlib onnxruntime onnx &> /dev/null\n",
        "\n",
        "# Download the ViT versions of the mobile SAM model weights from github repository.\n",
        "!git clone https://github.com/ChaoningZhang/MobileSAM.git\n",
        "%cd MobileSAM\n",
        "!pip install -e .\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv8rJFbangif",
        "outputId": "2d84e439-80a6-40ec-95e1-2289a9cdf28b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MobileSAM'...\n",
            "remote: Enumerating objects: 1011, done.\u001b[K\n",
            "remote: Counting objects: 100% (399/399), done.\u001b[K\n",
            "remote: Compressing objects: 100% (139/139), done.\u001b[K\n",
            "remote: Total 1011 (delta 344), reused 260 (delta 260), pack-reused 612 (from 3)\u001b[K\n",
            "Receiving objects: 100% (1011/1011), 95.45 MiB | 22.83 MiB/s, done.\n",
            "Resolving deltas: 100% (459/459), done.\n",
            "/content/MobileSAM\n",
            "Obtaining file:///content/MobileSAM\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing collected packages: mobile_sam\n",
            "  Running setup.py develop for mobile_sam\n",
            "Successfully installed mobile_sam-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === CONFIGURATION ===\n",
        "# Set the path to your training images and labels\n",
        "image_path = \"/content/drive/MyDrive/canal_project/27_11_2025/train/images\"   # <-- Replace with actual image folder path\n",
        "label_path = \"/content/drive/MyDrive/canal_project/27_11_2025/train/masks\"   # <-- Replace with actual label folder path\n",
        "\n",
        "# === Load Image Paths ===\n",
        "# Count total number of image files (e.g., .jpg format)\n",
        "all_image_paths = sorted(glob(os.path.join(image_path, \"*.jpg\")))  # Use .png if needed\n",
        "total_images = len(all_image_paths)\n",
        "print(f\"Total Number of Images: {total_images}\")\n",
        "\n",
        "# === Load Label Paths ===\n",
        "# Count total number of label files (e.g., .png format for segmentation masks)\n",
        "all_label_paths = sorted(glob(os.path.join(label_path, \"*.png\")))\n",
        "total_labels = len(all_label_paths)\n",
        "print(f\"Total Number of Labels: {total_labels}\")\n",
        "\n",
        "# === Match Images and Labels ===\n",
        "# Assuming both are in matching order and of equal length\n",
        "train_image_paths = all_image_paths[:total_images]\n",
        "train_label_paths = all_label_paths[:total_labels]\n",
        "\n",
        "# Preview label paths (for verification)\n",
        "print(\"Sample label paths:\")\n",
        "for path in train_label_paths[:5]:\n",
        "    print(path)"
      ],
      "metadata": {
        "id": "yQlhSIIYno6h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfc62cae-892a-46f0-b161-922163307d39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Number of Images: 186\n",
            "Total Number of Labels: 186\n",
            "Sample label paths:\n",
            "/content/drive/MyDrive/canal_project/27_11_2025/train/masks/image_capture_2025-04-05_11-45-56_jpg.rf.3d31effc5cd44f2feb5ca1c2c219346b_mask.png\n",
            "/content/drive/MyDrive/canal_project/27_11_2025/train/masks/image_capture_2025-04-06_10-10-41_jpg.rf.2d25833c6629824eb13665eb4b7a794c_mask.png\n",
            "/content/drive/MyDrive/canal_project/27_11_2025/train/masks/image_capture_2025-04-06_11-10-42_jpg.rf.ddc1a4d0ad9d75799e5114aa78022e19_mask.png\n",
            "/content/drive/MyDrive/canal_project/27_11_2025/train/masks/image_capture_2025-04-06_11-11-48_jpg.rf.6b1a6629a791c2c0c47e5ed49cca16ee_mask.png\n",
            "/content/drive/MyDrive/canal_project/27_11_2025/train/masks/image_capture_2025-04-06_12-12-56_jpg.rf.a087e1490f708cdb45e28555f3786aaf_mask.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === CONFIGURATION ===\n",
        "# Set the path to your validation images and labels\n",
        "val_image_path = \"/content/drive/MyDrive/canal_project/27_11_2025/valid/images\"   # <-- Replace with actual validation image folder\n",
        "val_label_path = \"/content/drive/MyDrive/canal_project/27_11_2025/valid/masks\"   # <-- Replace with actual validation label folder\n",
        "\n",
        "# === Load Validation Image Paths ===\n",
        "# Collect and sort all .jpg image files in the validation folder\n",
        "val_all_image_paths = sorted(glob(os.path.join(val_image_path, \"*.jpg\")))\n",
        "val_total_images = len(val_all_image_paths)\n",
        "print(f\"Total Number of Validation Images: {val_total_images}\")\n",
        "\n",
        "# === Load Validation Label Paths ===\n",
        "# Collect and sort all .png label files in the validation folder\n",
        "val_all_label_paths = sorted(glob(os.path.join(val_label_path, \"*.png\")))\n",
        "val_total_labels = len(val_all_label_paths)\n",
        "print(f\"Total Number of Validation Labels: {val_total_labels}\")\n",
        "\n",
        "# === Match Images and Labels (by order) ===\n",
        "# This assumes one-to-one correspondence between image and label files\n",
        "Val1_image_paths = val_all_image_paths[:val_total_images]\n",
        "Val1_label_paths = val_all_label_paths[:val_total_labels]\n",
        "\n",
        "# Preview a few label paths to confirm loading\n",
        "print(\"Sample validation label paths:\")\n",
        "for path in Val1_label_paths[:5]:\n",
        "    print(path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9O1MQTHL3yt",
        "outputId": "ac4983ec-c4ea-4c0a-bdb2-2be015701b67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Number of Validation Images: 39\n",
            "Total Number of Validation Labels: 39\n",
            "Sample validation label paths:\n",
            "/content/drive/MyDrive/canal_project/27_11_2025/valid/masks/image_capture_2025-04-05_11-44-49_jpg.rf.b321be12d6ab3078ae5064300cfb22ac_mask.png\n",
            "/content/drive/MyDrive/canal_project/27_11_2025/valid/masks/image_capture_2025-04-07_11-37-39_jpg.rf.2fabed3881bc90ecc487ee025e0509cb_mask.png\n",
            "/content/drive/MyDrive/canal_project/27_11_2025/valid/masks/image_capture_2025-04-08_11-04-35_jpg.rf.da8fbcd31f962ae95b3d5301f544de7e_mask.png\n",
            "/content/drive/MyDrive/canal_project/27_11_2025/valid/masks/image_capture_2025-04-09_11-31-33_jpg.rf.f831b52a22d28ffa7e8e351d3e28381b_mask.png\n",
            "/content/drive/MyDrive/canal_project/27_11_2025/valid/masks/image_capture_2025-05-19_12-47-22_jpg.rf.7deac398fbf7675b3cb73df0620b94d2_mask.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Please dont run this line if you would like to use the original size of input images.\n",
        "desired_size=(640, 640)\n"
      ],
      "metadata": {
        "id": "hy0gKyaOMfsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Load and Process Ground Truth Masks ===\n",
        "# This dictionary will store binary masks where pixel > 0 is treated as True\n",
        "ground_truth_masks = {}\n",
        "\n",
        "for idx in range(len(train_label_paths)):\n",
        "    # Read the label mask in grayscale\n",
        "    gt_grayscale = cv2.imread(train_label_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Resize the mask if desired_size is specified\n",
        "    if desired_size is not None:\n",
        "        gt_grayscale = cv2.resize(gt_grayscale, desired_size, interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    # Convert to binary mask (True where pixel > 0)\n",
        "    ground_truth_masks[idx] = (gt_grayscale > 0)\n",
        "\n",
        "# Optional: Print number of masks and preview a sample\n",
        "print(f\"Total ground truth masks loaded: {len(ground_truth_masks)}\")\n",
        "print(\"Example binary mask shape:\", ground_truth_masks[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgkZ0KCYMiZa",
        "outputId": "839a3591-2f22-405f-fe02-83bd5f83eddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total ground truth masks loaded: 186\n",
            "Example binary mask shape: (640, 640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Load and Process Validation Ground Truth Masks ===\n",
        "# This dictionary will store binary masks for validation data\n",
        "ground_truth_masksv = {}\n",
        "\n",
        "for idx in range(len(Val1_label_paths)):\n",
        "    # Read the validation label mask in grayscale\n",
        "    gt_grayscale = cv2.imread(Val1_label_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Resize the mask if a desired size is specified\n",
        "    if desired_size is not None:\n",
        "        gt_grayscale = cv2.resize(gt_grayscale, desired_size, interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    # Convert to binary mask: True where pixel > 0\n",
        "    ground_truth_masksv[idx] = (gt_grayscale > 0)\n",
        "\n",
        "# Print summary\n",
        "print(f\"Total validation ground truth masks loaded: {len(ground_truth_masksv)}\")\n",
        "print(\"Example validation mask shape:\", ground_truth_masksv[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjeXEcLqMltk",
        "outputId": "26f42d23-ba3e-4bb7-ea1e-6d4a5da5e000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total validation ground truth masks loaded: 39\n",
            "Example validation mask shape: (640, 640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_type = \"vit_t\"\n",
        "checkpoint = \"/content/MobileSAM/weights/mobile_sam.pt\"\n",
        "device = 'cuda:0'"
      ],
      "metadata": {
        "id": "8SyxlGx1MyDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/ChaoningZhang/MobileSAM/raw/master/weights/mobile_sam.pt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tfe3jMmdN28V",
        "outputId": "de1caf1b-cafb-45bd-ac9d-a87a980a8397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-29 21:48:43--  https://github.com/ChaoningZhang/MobileSAM/raw/master/weights/mobile_sam.pt\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ChaoningZhang/MobileSAM/master/weights/mobile_sam.pt [following]\n",
            "--2025-11-29 21:48:43--  https://raw.githubusercontent.com/ChaoningZhang/MobileSAM/master/weights/mobile_sam.pt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 40728226 (39M) [application/octet-stream]\n",
            "Saving to: ‘mobile_sam.pt’\n",
            "\n",
            "mobile_sam.pt       100%[===================>]  38.84M  66.6MB/s    in 0.6s    \n",
            "\n",
            "2025-11-29 21:48:46 (66.6 MB/s) - ‘mobile_sam.pt’ saved [40728226/40728226]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from mobile_sam import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
        "import torch\n",
        "\n",
        "# === Load Pretrained SAM Model ===\n",
        "sam_model = sam_model_registry[model_type](checkpoint=checkpoint)\n",
        "\n",
        "# === Device Setup ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "sam_model.to(device)\n",
        "\n",
        "# === Set to Training Mode/Inferencing Mode (for fine-tuning) ===\n",
        "sam_model.eval()\n",
        "sam_model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlKXRepAM2g3",
        "outputId": "130e73f8-39c7-4be1-e63f-a93f13909b52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/content/MobileSAM/mobile_sam/modeling/tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_5m_224 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  return register_model(fn_wrapper)\n",
            "/content/MobileSAM/mobile_sam/modeling/tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_11m_224 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  return register_model(fn_wrapper)\n",
            "/content/MobileSAM/mobile_sam/modeling/tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_21m_224 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  return register_model(fn_wrapper)\n",
            "/content/MobileSAM/mobile_sam/modeling/tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_21m_384 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  return register_model(fn_wrapper)\n",
            "/content/MobileSAM/mobile_sam/modeling/tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_21m_512 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  return register_model(fn_wrapper)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sam(\n",
              "  (image_encoder): TinyViT(\n",
              "    (patch_embed): PatchEmbed(\n",
              "      (seq): Sequential(\n",
              "        (0): Conv2d_BN(\n",
              "          (c): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Conv2d_BN(\n",
              "          (c): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layers): ModuleList(\n",
              "      (0): ConvLayer(\n",
              "        (blocks): ModuleList(\n",
              "          (0-1): 2 x MBConv(\n",
              "            (conv1): Conv2d_BN(\n",
              "              (c): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (act1): GELU(approximate='none')\n",
              "            (conv2): Conv2d_BN(\n",
              "              (c): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (act2): GELU(approximate='none')\n",
              "            (conv3): Conv2d_BN(\n",
              "              (c): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (act3): GELU(approximate='none')\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "        )\n",
              "        (downsample): PatchMerging(\n",
              "          (act): GELU(approximate='none')\n",
              "          (conv1): Conv2d_BN(\n",
              "            (c): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (conv2): Conv2d_BN(\n",
              "            (c): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
              "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (conv3): Conv2d_BN(\n",
              "            (c): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): BasicLayer(\n",
              "        dim=128, input_resolution=(128, 128), depth=2\n",
              "        (blocks): ModuleList(\n",
              "          (0-1): 2 x TinyViTBlock(\n",
              "            dim=128, input_resolution=(128, 128), num_heads=4, window_size=7, mlp_ratio=4.0\n",
              "            (drop_path): Identity()\n",
              "            (attn): Attention(\n",
              "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
              "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (mlp): Mlp(\n",
              "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
              "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (local_conv): Conv2d_BN(\n",
              "              (c): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
              "              (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (downsample): PatchMerging(\n",
              "          (act): GELU(approximate='none')\n",
              "          (conv1): Conv2d_BN(\n",
              "            (c): Conv2d(128, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (conv2): Conv2d_BN(\n",
              "            (c): Conv2d(160, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=160, bias=False)\n",
              "            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (conv3): Conv2d_BN(\n",
              "            (c): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): BasicLayer(\n",
              "        dim=160, input_resolution=(64, 64), depth=6\n",
              "        (blocks): ModuleList(\n",
              "          (0-5): 6 x TinyViTBlock(\n",
              "            dim=160, input_resolution=(64, 64), num_heads=5, window_size=14, mlp_ratio=4.0\n",
              "            (drop_path): Identity()\n",
              "            (attn): Attention(\n",
              "              (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
              "              (qkv): Linear(in_features=160, out_features=480, bias=True)\n",
              "              (proj): Linear(in_features=160, out_features=160, bias=True)\n",
              "            )\n",
              "            (mlp): Mlp(\n",
              "              (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
              "              (fc1): Linear(in_features=160, out_features=640, bias=True)\n",
              "              (fc2): Linear(in_features=640, out_features=160, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (local_conv): Conv2d_BN(\n",
              "              (c): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
              "              (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (downsample): PatchMerging(\n",
              "          (act): GELU(approximate='none')\n",
              "          (conv1): Conv2d_BN(\n",
              "            (c): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (conv2): Conv2d_BN(\n",
              "            (c): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
              "            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (conv3): Conv2d_BN(\n",
              "            (c): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): BasicLayer(\n",
              "        dim=320, input_resolution=(64, 64), depth=2\n",
              "        (blocks): ModuleList(\n",
              "          (0-1): 2 x TinyViTBlock(\n",
              "            dim=320, input_resolution=(64, 64), num_heads=10, window_size=7, mlp_ratio=4.0\n",
              "            (drop_path): Identity()\n",
              "            (attn): Attention(\n",
              "              (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (qkv): Linear(in_features=320, out_features=960, bias=True)\n",
              "              (proj): Linear(in_features=320, out_features=320, bias=True)\n",
              "            )\n",
              "            (mlp): Mlp(\n",
              "              (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
              "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (local_conv): Conv2d_BN(\n",
              "              (c): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
              "              (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm_head): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "    (head): Linear(in_features=320, out_features=1000, bias=True)\n",
              "    (neck): Sequential(\n",
              "      (0): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): LayerNorm2d()\n",
              "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (3): LayerNorm2d()\n",
              "    )\n",
              "  )\n",
              "  (prompt_encoder): PromptEncoder(\n",
              "    (pe_layer): PositionEmbeddingRandom()\n",
              "    (point_embeddings): ModuleList(\n",
              "      (0-3): 4 x Embedding(1, 256)\n",
              "    )\n",
              "    (not_a_point_embed): Embedding(1, 256)\n",
              "    (mask_downscaling): Sequential(\n",
              "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (1): LayerNorm2d()\n",
              "      (2): GELU(approximate='none')\n",
              "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (4): LayerNorm2d()\n",
              "      (5): GELU(approximate='none')\n",
              "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (no_mask_embed): Embedding(1, 256)\n",
              "  )\n",
              "  (mask_decoder): MaskDecoder(\n",
              "    (transformer): TwoWayTransformer(\n",
              "      (layers): ModuleList(\n",
              "        (0-1): 2 x TwoWayAttentionBlock(\n",
              "          (self_attn): Attention(\n",
              "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (cross_attn_token_to_image): Attention(\n",
              "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
              "          )\n",
              "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "            (act): ReLU()\n",
              "          )\n",
              "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (cross_attn_image_to_token): Attention(\n",
              "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (final_attn_token_to_image): Attention(\n",
              "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
              "      )\n",
              "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (iou_token): Embedding(1, 256)\n",
              "    (mask_tokens): Embedding(4, 256)\n",
              "    (output_upscaling): Sequential(\n",
              "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (1): LayerNorm2d()\n",
              "      (2): GELU(approximate='none')\n",
              "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (4): GELU(approximate='none')\n",
              "    )\n",
              "    (output_hypernetworks_mlps): ModuleList(\n",
              "      (0-3): 4 x MLP(\n",
              "        (layers): ModuleList(\n",
              "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
              "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (iou_prediction_head): MLP(\n",
              "      (layers): ModuleList(\n",
              "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
              "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segment-anything &> /dev/null"
      ],
      "metadata": {
        "id": "KtVKHbsMON7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from segment_anything.utils.transforms import ResizeLongestSide\n",
        "\n",
        "# === Initialize Resize Transformation ===\n",
        "# This ensures images and masks are resized while preserving aspect ratio\n",
        "transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
        "\n",
        "# === Containers for Processed Data ===\n",
        "transformed_data = defaultdict(dict)  # holds resized + preprocessed inputs for SAM\n",
        "ground_truth_masks = {}               # holds binary masks for training\n",
        "\n",
        "# === Preprocess All Training Samples ===\n",
        "for k in range(len(train_image_paths)):\n",
        "    # Load RGB image and grayscale ground truth mask\n",
        "    image_bgr = cv2.imread(train_image_paths[k])\n",
        "    mask_gray = cv2.imread(train_label_paths[k], cv2.IMREAD_GRAYSCALE)\n",
        "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Apply resizing transformation to both image and mask\n",
        "    resized_image = transform.apply_image(image_rgb)\n",
        "    resized_mask = transform.apply_image(mask_gray)\n",
        "\n",
        "    # Convert image to torch tensor and format as [1, 3, H, W]\n",
        "    image_tensor = torch.as_tensor(resized_image, device=device).permute(2, 0, 1).unsqueeze(0).contiguous()\n",
        "\n",
        "    # Preprocess for SAM (normalization, padding, etc.)\n",
        "    input_tensor = sam_model.preprocess(image_tensor)\n",
        "\n",
        "    # Store processed image and metadata\n",
        "    transformed_data[k]['image'] = input_tensor\n",
        "    transformed_data[k]['input_size'] = resized_image.shape[:2][::-1]  # (W, H)\n",
        "    transformed_data[k]['original_image_size'] = image_rgb.shape[:2]   # (H, W)\n",
        "\n",
        "    # Store resized mask as binary (bool)\n",
        "    ground_truth_masks[k] = (resized_mask > 0)\n",
        "\n",
        "print(f\"Preprocessed {len(transformed_data)} training samples.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhwKOOwcNFSH",
        "outputId": "b7bc6daf-f224-47b8-c6e8-00ac163c633e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed 186 training samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mobile_sam import SamPredictor\n",
        "\n",
        "sam_model.to(device)\n",
        "predictor_tuned = SamPredictor(sam_model)\n"
      ],
      "metadata": {
        "id": "kxJVPKwrPLRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Initialize Resize Transformation ===\n",
        "transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
        "\n",
        "# === Storage for Results ===\n",
        "ground_truth_masksv = {}   # Ground truth validation masks (resized and binarized)\n",
        "images_tuned_list = {}     # Resized RGB images used for prediction\n",
        "masks_tuned_list = {}      # Predicted masks from SAM\n",
        "\n",
        "# === Loop Through Validation Images ===\n",
        "for s in range(len(Val1_image_paths)):\n",
        "    # Load RGB image and grayscale ground truth mask\n",
        "    image_bgr = cv2.imread(Val1_image_paths[s])\n",
        "    gt_mask = cv2.imread(Val1_label_paths[s], cv2.IMREAD_GRAYSCALE)\n",
        "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Resize image and ground truth using SAM’s aspect-preserving transform\n",
        "    resized_image = transform.apply_image(image_rgb)\n",
        "    resized_mask = transform.apply_image(gt_mask)\n",
        "\n",
        "    # Set resized image for SAM prediction\n",
        "    predictor_tuned.set_image(resized_image)\n",
        "\n",
        "    # Predict masks (without prompt)\n",
        "    masks_tuned, _, _ = predictor_tuned.predict(\n",
        "        point_coords=None,\n",
        "        box=None,\n",
        "        multimask_output=False\n",
        "    )\n",
        "\n",
        "    # Convert SAM prediction to binary mask\n",
        "    pred_mask = masks_tuned[0]\n",
        "    binary_mask = (pred_mask > 0).astype(np.float32)\n",
        "\n",
        "    # Store outputs\n",
        "    images_tuned_list[s] = resized_image\n",
        "    masks_tuned_list[s] = binary_mask\n",
        "    ground_truth_masksv[s] = (resized_mask > 0).astype(np.float32)\n",
        "\n",
        "print(f\"Predictions complete for {len(Val1_image_paths)} validation images.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PLLxPm_OzsH",
        "outputId": "a0e3538c-f7c5-4bb8-bf74-9054d0af3bb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions complete for 39 validation images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Training Configuration ===\n",
        "keys = list(ground_truth_masks.keys())  # List of training sample indices\n",
        "batch_size = 32                         # Adjust based on available GPU memory\n",
        "num_epochs = 5                          # Total number of training epochs\n",
        "\n",
        "# === Parameters to Fine-Tune ===\n",
        "# You can choose to fine-tune all components or a subset\n",
        "params_to_optimize = (\n",
        "    list(sam_model.mask_decoder.parameters()) +        # Mask decoder (usually the main fine-tuning target)\n",
        "    list(sam_model.image_encoder.parameters()) +       # Optional: include image encoder\n",
        "    list(sam_model.prompt_encoder.parameters())        # Optional: include prompt encoder\n",
        ")\n",
        "\n",
        "# === Optimizer ===\n",
        "# Adam optimizer with low learning rate for stable fine-tuning\n",
        "optimizer = torch.optim.Adam(params_to_optimize, lr=1e-4, weight_decay=0)\n",
        "\n",
        "# === Loss Function ===\n",
        "# Binary Cross-Entropy with logits is used for binary segmentation tasks\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "ysyxjrjmPcs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# === Validation DataLoader Setup ===\n",
        "# Here we're using a list of file paths as the dataset, which will later need to be wrapped in a proper Dataset class\n",
        "val_loader = DataLoader(Val1_image_paths, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# === Basic Validation Dataset Checks ===\n",
        "\n",
        "# Total number of validation examples\n",
        "num_val_examples = len(Val1_image_paths)\n",
        "print(f\"Number of validation examples: {num_val_examples}\")\n",
        "\n",
        "# Number of items returned by val_loader.dataset (same as above since it's a list)\n",
        "print(f\"Number of examples in validation dataset (via DataLoader): {len(val_loader.dataset)}\")\n",
        "\n",
        "# Number of batches in the validation DataLoader\n",
        "print(f\"Number of batches in validation loader: {len(val_loader)}\")\n",
        "\n",
        "# === Safety Check ===\n",
        "# Prevent training from continuing if validation data is empty\n",
        "if num_val_examples == 0:\n",
        "    raise ValueError(\"The validation dataset is empty. Please check your data paths.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0Jog2GoPe9h",
        "outputId": "31ad78f6-c3ba-4ea3-fbed-506523d48227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of validation examples: 39\n",
            "Number of examples in validation dataset (via DataLoader): 39\n",
            "Number of batches in validation loader: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "w41bfe-TPi1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Accuracy Calculation Function ===\n",
        "def calculate_accuracy(predictions, targets):\n",
        "    \"\"\"\n",
        "    Computes binary accuracy between predicted and ground truth masks.\n",
        "    \"\"\"\n",
        "    binary_predictions = (predictions > 0.5).float()\n",
        "    accuracy = (binary_predictions == targets).float().mean()\n",
        "    return accuracy.item()\n",
        "\n",
        "# === Batch Training Function ===\n",
        "def train_on_batch(keys, batch_start, batch_end):\n",
        "    \"\"\"\n",
        "    Trains the SAM mask decoder on a batch of images and masks.\n",
        "    Returns batch loss and accuracy.\n",
        "    \"\"\"\n",
        "    batch_losses = []\n",
        "    batch_accuracies = []\n",
        "\n",
        "    for k in keys[batch_start:batch_end]:\n",
        "        # === Get input data and metadata\n",
        "        input_image = transformed_data[k]['image'].to(device)\n",
        "        input_size = transformed_data[k]['input_size']\n",
        "        original_image_size = transformed_data[k]['original_image_size']\n",
        "\n",
        "        # === Forward Pass (frozen encoders)\n",
        "        with torch.no_grad():\n",
        "            image_embedding = sam_model.image_encoder(input_image)\n",
        "            sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n",
        "                points=None, boxes=None, masks=None\n",
        "            )\n",
        "\n",
        "        low_res_masks, _ = sam_model.mask_decoder(\n",
        "            image_embeddings=image_embedding,\n",
        "            image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
        "            sparse_prompt_embeddings=sparse_embeddings,\n",
        "            dense_prompt_embeddings=dense_embeddings,\n",
        "            multimask_output=False\n",
        "        )\n",
        "\n",
        "        # === Resize prediction to original size\n",
        "        upscaled_masks = sam_model.postprocess_masks(\n",
        "            low_res_masks, input_size, original_image_size\n",
        "        ).to(device)\n",
        "\n",
        "        # === Resize ground truth mask to match output\n",
        "        gt_np = ground_truth_masks[k].astype(np.uint8)\n",
        "        resized_gt = cv2.resize(gt_np, upscaled_masks.shape[-2:][::-1], interpolation=cv2.INTER_NEAREST)\n",
        "        gt_binary_mask = torch.tensor(resized_gt, dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "        # === Compute loss and update weights\n",
        "        loss = loss_fn(upscaled_masks, gt_binary_mask)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # === Record metrics\n",
        "        batch_losses.append(loss.item())\n",
        "        batch_accuracies.append(calculate_accuracy(torch.sigmoid(upscaled_masks), gt_binary_mask))\n",
        "\n",
        "    return batch_losses, batch_accuracies\n",
        "\n",
        "# === Epoch Training Loop ===\n",
        "losses, accuracies = [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_losses = []\n",
        "    epoch_accuracies = []\n",
        "\n",
        "    print(f\"\\n--- EPOCH {epoch + 1}/{num_epochs} ---\")\n",
        "\n",
        "    for batch_start in range(0, len(keys), batch_size):\n",
        "        batch_end = min(batch_start + batch_size, len(keys))\n",
        "        batch_losses, batch_accuracies = train_on_batch(keys, batch_start, batch_end)\n",
        "\n",
        "        batch_loss = mean(batch_losses)\n",
        "        batch_accuracy = mean(batch_accuracies)\n",
        "        epoch_losses.append(batch_loss)\n",
        "        epoch_accuracies.extend(batch_accuracies)\n",
        "\n",
        "        print(f'Batch [{batch_start + 1}–{batch_end}] | Loss: {batch_loss:.6f} | Accuracy: {batch_accuracy:.4f}')\n",
        "\n",
        "    # === End of Epoch ===\n",
        "    mean_train_loss = mean(epoch_losses)\n",
        "    mean_train_accuracy = mean(epoch_accuracies)\n",
        "    losses.append(mean_train_loss)\n",
        "    accuracies.append(mean_train_accuracy)\n",
        "\n",
        "    print(f'\\nEpoch {epoch + 1} Summary:')\n",
        "    print(f'➤ Mean Training Loss: {mean_train_loss:.6f}')\n",
        "    print(f'➤ Mean Training Accuracy: {mean_train_accuracy:.4f}')\n",
        "\n",
        "    # Clear cache to manage memory\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "pTeUDh5APli9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8d0de3f-42a8-4ffb-94e5-4e82ef34a28c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- EPOCH 1/5 ---\n",
            "Batch [1–32] | Loss: 0.085612 | Accuracy: 0.9641\n",
            "Batch [33–64] | Loss: 0.012700 | Accuracy: 0.9957\n",
            "Batch [65–96] | Loss: 0.018362 | Accuracy: 0.9929\n",
            "Batch [97–128] | Loss: 0.017522 | Accuracy: 0.9936\n",
            "Batch [129–160] | Loss: 0.018596 | Accuracy: 0.9939\n",
            "Batch [161–186] | Loss: 0.011916 | Accuracy: 0.9953\n",
            "\n",
            "Epoch 1 Summary:\n",
            "➤ Mean Training Loss: 0.027451\n",
            "➤ Mean Training Accuracy: 0.9891\n",
            "\n",
            "--- EPOCH 2/5 ---\n",
            "Batch [1–32] | Loss: 0.013629 | Accuracy: 0.9949\n",
            "Batch [33–64] | Loss: 0.009086 | Accuracy: 0.9963\n",
            "Batch [65–96] | Loss: 0.012932 | Accuracy: 0.9949\n",
            "Batch [97–128] | Loss: 0.012691 | Accuracy: 0.9951\n",
            "Batch [129–160] | Loss: 0.015278 | Accuracy: 0.9936\n",
            "Batch [161–186] | Loss: 0.010367 | Accuracy: 0.9959\n",
            "\n",
            "Epoch 2 Summary:\n",
            "➤ Mean Training Loss: 0.012330\n",
            "➤ Mean Training Accuracy: 0.9951\n",
            "\n",
            "--- EPOCH 3/5 ---\n",
            "Batch [1–32] | Loss: 0.011319 | Accuracy: 0.9955\n",
            "Batch [33–64] | Loss: 0.008311 | Accuracy: 0.9966\n",
            "Batch [65–96] | Loss: 0.012422 | Accuracy: 0.9950\n",
            "Batch [97–128] | Loss: 0.011673 | Accuracy: 0.9954\n",
            "Batch [129–160] | Loss: 0.012575 | Accuracy: 0.9947\n",
            "Batch [161–186] | Loss: 0.009523 | Accuracy: 0.9962\n",
            "\n",
            "Epoch 3 Summary:\n",
            "➤ Mean Training Loss: 0.010971\n",
            "➤ Mean Training Accuracy: 0.9956\n",
            "\n",
            "--- EPOCH 4/5 ---\n",
            "Batch [1–32] | Loss: 0.010250 | Accuracy: 0.9959\n",
            "Batch [33–64] | Loss: 0.007900 | Accuracy: 0.9968\n",
            "Batch [65–96] | Loss: 0.011282 | Accuracy: 0.9955\n",
            "Batch [97–128] | Loss: 0.011213 | Accuracy: 0.9955\n",
            "Batch [129–160] | Loss: 0.010599 | Accuracy: 0.9957\n",
            "Batch [161–186] | Loss: 0.009078 | Accuracy: 0.9963\n",
            "\n",
            "Epoch 4 Summary:\n",
            "➤ Mean Training Loss: 0.010054\n",
            "➤ Mean Training Accuracy: 0.9959\n",
            "\n",
            "--- EPOCH 5/5 ---\n",
            "Batch [1–32] | Loss: 0.009766 | Accuracy: 0.9962\n",
            "Batch [33–64] | Loss: 0.007740 | Accuracy: 0.9969\n",
            "Batch [65–96] | Loss: 0.009977 | Accuracy: 0.9960\n",
            "Batch [97–128] | Loss: 0.010413 | Accuracy: 0.9958\n",
            "Batch [129–160] | Loss: 0.010312 | Accuracy: 0.9958\n",
            "Batch [161–186] | Loss: 0.008611 | Accuracy: 0.9965\n",
            "\n",
            "Epoch 5 Summary:\n",
            "➤ Mean Training Loss: 0.009470\n",
            "➤ Mean Training Accuracy: 0.9962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(type(ground_truth_masksv))\n",
        "print(ground_truth_masksv.keys() if isinstance(ground_truth_masksv, dict) else len(ground_truth_masksv))"
      ],
      "metadata": {
        "id": "zcSMpqOHPv2S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c89a98b-6bbf-455f-a018-ad3a60bbaaac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Set the paths to your test images and labels\n",
        "test_image_dir = \"/content/drive/MyDrive/canal_project/27_11_2025/test/images\"\n",
        "test_label_dir = \"/content/drive/MyDrive/canal_project/27_11_2025/test/masks\"\n",
        "\n",
        "# === Load and Sort Test Image Paths ===\n",
        "all_test_image_paths = sorted(glob(os.path.join(test_image_dir, \"*.jpg\")))\n",
        "print(f\"Total Number of Test Images: {len(all_test_image_paths)}\")\n",
        "\n",
        "# === Load and Sort Test Label Paths ===\n",
        "all_test_label_paths = sorted(glob(os.path.join(test_label_dir, \"*.png\")))\n",
        "print(f\"Total Number of Test Labels: {len(all_test_label_paths)}\")\n",
        "\n",
        "# === Match Image and Label Paths ===\n",
        "Test_image_paths = all_test_image_paths\n",
        "Test_label_paths = all_test_label_paths\n",
        "\n",
        "# === Count test samples (NOW IT IS SAFE) ===\n",
        "test_total_images = len(Test_image_paths)\n",
        "test_total_labels = len(Test_label_paths)\n",
        "\n",
        "# === Dictionary to store binary test masks ===\n",
        "ground_truth_test_masks = {}\n",
        "\n",
        "# If you have a desired resizing size\n",
        "desired_size = None   # or (512, 512)\n",
        "\n",
        "# === Load and resize ground truth masks ===\n",
        "for k in range(test_total_labels):\n",
        "    gt_gray = cv2.imread(Test_label_paths[k], cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    if desired_size is not None:\n",
        "        gt_gray = cv2.resize(gt_gray, desired_size, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    ground_truth_test_masks[k] = (gt_gray > 0).astype(np.float32)\n",
        "\n",
        "print(f\"Loaded {len(ground_truth_test_masks)} ground truth test masks.\")\n",
        "\n",
        "# === Print sample paths ===\n",
        "print(\"Sample test image path:\", Test_image_paths[0] if Test_image_paths else \"No images found\")\n",
        "print(\"Sample test label path:\", Test_label_paths[0] if Test_label_paths else \"No labels found\")\n"
      ],
      "metadata": {
        "id": "eALMCZbuP1T7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b35f73f5-7a59-4e77-a4fe-6b83f9d70542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Number of Test Images: 40\n",
            "Total Number of Test Labels: 40\n",
            "Loaded 40 ground truth test masks.\n",
            "Sample test image path: /content/drive/MyDrive/canal_project/27_11_2025/test/images/image_capture_2025-04-06_12-11-49_jpg.rf.826be63e91371c207d97f6472d9d495b.jpg\n",
            "Sample test label path: /content/drive/MyDrive/canal_project/27_11_2025/test/masks/image_capture_2025-04-06_12-11-49_jpg.rf.826be63e91371c207d97f6472d9d495b_mask.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dictionary to hold ground truth binary masks for test data\n",
        "ground_truth_test_masks = {}\n",
        "\n",
        "# === Load and Process Each Test Mask ===\n",
        "for idx in range(len(Test_label_paths)):\n",
        "    # Read label image in color (3-channel); expected mask is in the red channel\n",
        "    gt_color = cv2.imread(Test_label_paths[idx])\n",
        "\n",
        "    # Extract the red channel only and convert to binary mask\n",
        "    # Note: OpenCV loads in BGR, so red is at index 2\n",
        "    binary_mask = (gt_color[:, :, 2] > 0).astype(np.float32)\n",
        "\n",
        "    # Resize if specified\n",
        "    if desired_size is not None:\n",
        "        binary_mask = cv2.resize(binary_mask, desired_size, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    # Store in dictionary\n",
        "    ground_truth_test_masks[idx] = binary_mask\n",
        "\n",
        "print(f\"Loaded {len(ground_truth_test_masks)} ground truth test masks.\")"
      ],
      "metadata": {
        "id": "uTEVaxrdQh0C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5509e3f9-3521-4f67-a5a1-d02659046c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 40 ground truth test masks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# === Output containers ===\n",
        "images_tuned_list = {}  # Stores resized RGB input images\n",
        "masks_tuned_list = {}   # Stores binary segmentation masks\n",
        "\n",
        "# === Run inference on all test images ===\n",
        "for k in range(test_total_images):\n",
        "    # Load image from disk\n",
        "    image_bgr = cv2.imread(Test_image_paths[k])  # use Test_image_paths here\n",
        "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Optional resizing\n",
        "    if desired_size is not None:\n",
        "        image_rgb = cv2.resize(image_rgb, desired_size, interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    # Set image for SAM predictor\n",
        "    predictor_tuned.set_image(image_rgb)\n",
        "\n",
        "    # Run prediction (no prompt input)\n",
        "    masks_tuned, _, _ = predictor_tuned.predict(\n",
        "        point_coords=None,\n",
        "        box=None,\n",
        "        multimask_output=False  # Only return most confident mask\n",
        "    )\n",
        "\n",
        "    # Convert first predicted mask to binary\n",
        "    pred_mask = masks_tuned[0]  # shape: (H, W)\n",
        "    binary_mask = (pred_mask > 0).astype(np.float32)\n",
        "\n",
        "    # Store results\n",
        "    images_tuned_list[k] = image_rgb\n",
        "    masks_tuned_list[k] = binary_mask\n",
        "\n",
        "print(f\"Inference complete on {test_total_images} test images.\")"
      ],
      "metadata": {
        "id": "hyOTZfFOQqlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# === Configuration ===\n",
        "n_images = len(images_tuned_list)\n",
        "n_cols = 4  # Images per row\n",
        "n_rows = (n_images + n_cols - 1) // n_cols  # Automatically calculate needed rows\n",
        "\n",
        "# === Create subplot grid ===\n",
        "fig, axs = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))\n",
        "\n",
        "# Ensure axs is always 2D (e.g., when n_rows = 1)\n",
        "axs = np.atleast_2d(axs)\n",
        "\n",
        "# === Loop through and display each image + overlay ===\n",
        "for i in range(n_rows):\n",
        "    for j in range(n_cols):\n",
        "        index = i * n_cols + j\n",
        "        ax = axs[i, j]\n",
        "\n",
        "        if index < n_images:\n",
        "            # Load image and binary mask\n",
        "            image = images_tuned_list[index]\n",
        "            mask = masks_tuned_list[index]\n",
        "\n",
        "            # Display the RGB image\n",
        "            ax.imshow(image, interpolation='none')\n",
        "\n",
        "            # Create blue overlay for mask\n",
        "            blue_overlay = np.stack([\n",
        "                np.zeros_like(mask),\n",
        "                np.zeros_like(mask),\n",
        "                (mask > 0).astype(np.float32)\n",
        "            ], axis=-1)\n",
        "\n",
        "            # Overlay with transparency\n",
        "            ax.imshow(blue_overlay, alpha=0.5)\n",
        "\n",
        "        # Turn off axes for visual cleanliness\n",
        "        ax.axis('off')\n",
        "\n",
        "# === Adjust layout ===\n",
        "plt.subplots_adjust(wspace=0.03, hspace=0.03)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lwMSb-F-QzdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import auc, roc_curve\n",
        "\n",
        "# === Binary Metrics for One Prediction ===\n",
        "def binary_segmentation_metrics(predictions, targets):\n",
        "    \"\"\"\n",
        "    Computes binary segmentation metrics for a single predicted mask vs ground truth.\n",
        "    Inputs:\n",
        "        predictions (numpy array): predicted mask, float32, range [0,1] or binary\n",
        "        targets (numpy array): ground truth mask, binary (0 or 1)\n",
        "    Returns:\n",
        "        Tuple of metrics: accuracy, precision, recall, F1-score, IoU, kappa, FP, FN, TP, TN, dice\n",
        "    \"\"\"\n",
        "    # Flatten and convert to binary\n",
        "    predictions = predictions.squeeze()\n",
        "    targets = targets.squeeze()\n",
        "\n",
        "    predictions_binary = (predictions > 0.5).astype(int)\n",
        "    targets_binary = targets.astype(int)\n",
        "\n",
        "    # Confusion matrix components\n",
        "    TP = np.sum((predictions_binary == 1) & (targets_binary == 1))\n",
        "    FP = np.sum((predictions_binary == 1) & (targets_binary == 0))\n",
        "    FN = np.sum((predictions_binary == 0) & (targets_binary == 1))\n",
        "    TN = np.sum((predictions_binary == 0) & (targets_binary == 0))\n",
        "\n",
        "    # Metrics with small epsilon to avoid division by zero\n",
        "    eps = 1e-5\n",
        "    accuracy = (TP + TN + eps) / (TP + FP + FN + TN + eps)\n",
        "    precision = (TP + eps) / (TP + FP + eps)\n",
        "    recall = (TP + eps) / (TP + FN + eps)\n",
        "    f_score = 2 * (precision * recall) / (precision + recall + eps)\n",
        "    dice = (2 * TP + eps) / (2 * TP + FP + FN + eps)\n",
        "    iou = (TP + eps) / (TP + FP + FN + eps)\n",
        "\n",
        "    # Cohen’s kappa\n",
        "    total = TP + FP + FN + TN\n",
        "    p_o = (TP + TN) / total\n",
        "    p_e = ((TP + FP) * (TP + FN) + (FN + TN) * (FP + TN)) / (total ** 2)\n",
        "    kappa = (p_o - p_e) / (1 - p_e + eps)\n",
        "\n",
        "    return accuracy, precision, recall, f_score, iou, kappa, FP, FN, TP, TN, dice\n",
        "\n",
        "# === Average Metrics Across Dataset ===\n",
        "def calculate_average_metrics(predictions_list, targets_list):\n",
        "    \"\"\"\n",
        "    Computes average binary segmentation metrics across a dataset.\n",
        "    Inputs:\n",
        "        predictions_list: dictionary or list of predicted masks\n",
        "        targets_list: dictionary or list of ground truth masks\n",
        "    Returns:\n",
        "        Dictionary of averaged metrics\n",
        "    \"\"\"\n",
        "    num_masks = len(predictions_list)\n",
        "\n",
        "    total_metrics = {\n",
        "        'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f_score': 0.0,\n",
        "        'iou': 0.0, 'kappa': 0.0, 'FP': 0, 'FN': 0, 'MAR': 0.0, 'FAR': 0.0, 'dice': 0.0\n",
        "    }\n",
        "\n",
        "    for i in range(num_masks):\n",
        "        pred = predictions_list[i]\n",
        "        gt = targets_list[i]\n",
        "        metrics = binary_segmentation_metrics(pred, gt)\n",
        "\n",
        "        # Accumulate each metric\n",
        "        for metric_name, value in zip(total_metrics.keys(), metrics):\n",
        "            total_metrics[metric_name] += value\n",
        "\n",
        "        # Add False Negative Rate (Missed Alarm Rate, MAR) and False Alarm Rate (FAR)\n",
        "        TP, TN, FP, FN = metrics[8], metrics[9], metrics[6], metrics[7]\n",
        "        total_metrics['MAR'] += FN / (FN + TP + 1e-5)\n",
        "        total_metrics['FAR'] += FP / (FP + TN + 1e-5)\n",
        "\n",
        "    # Compute mean for each metric\n",
        "    avg_metrics = {k: v / num_masks for k, v in total_metrics.items()}\n",
        "\n",
        "    return avg_metrics\n",
        "\n",
        "# === Example Usage ===\n",
        "# Evaluate the SAM predictions vs. ground truth test masks\n",
        "avg_metrics = calculate_average_metrics(masks_tuned_list, ground_truth_test_masks)\n",
        "\n",
        "# Print results\n",
        "print(\"\\n=== Average Metrics on Test Set ===\")\n",
        "for metric_name, value in avg_metrics.items():\n",
        "    print(f\"{metric_name.upper():<8}: {value:.4f}\")"
      ],
      "metadata": {
        "id": "lJH6dxwlQ2M5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "from skimage.color import rgb2gray\n",
        "import math\n",
        "\n",
        "# === Configuration ===\n",
        "border_color = 'black'\n",
        "border_width = 0.35\n",
        "alpha_gray = 0.5  # transparency for base grayscale image\n",
        "alpha_tp = 0.5    # transparency for True Positives (blue)\n",
        "alpha_fa = 0.6    # False Alarms (green)\n",
        "alpha_ma = 0.6    # Missed Alarms (magenta)\n",
        "\n",
        "# === Determine Grid Layout ===\n",
        "num_images = len(images_tuned_list)\n",
        "cols = 4\n",
        "rows = math.ceil(num_images / cols)\n",
        "\n",
        "# === Setup Plot ===\n",
        "fig, axs = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows))\n",
        "axs = axs.flatten()  # flatten 2D axes array for 1D indexing\n",
        "\n",
        "# === Choose Which Images to Display (all by default) ===\n",
        "selected_indices = list(range(num_images))\n",
        "\n",
        "# === Visualization Loop ===\n",
        "for i, index in enumerate(selected_indices):\n",
        "    if index >= num_images:\n",
        "        break\n",
        "\n",
        "    # Convert image to grayscale\n",
        "    gray_image = rgb2gray(images_tuned_list[index])\n",
        "\n",
        "    # Get predicted and ground truth masks\n",
        "    pred_mask = masks_tuned_list[index]\n",
        "    gt_mask = ground_truth_test_masks[index]\n",
        "\n",
        "    ax = axs[i]\n",
        "    ax.imshow(gray_image, cmap='gray', interpolation='none', alpha=alpha_gray)\n",
        "\n",
        "    # === Mask Overlays ===\n",
        "\n",
        "    # True Positives: prediction and ground truth both 1 (blue)\n",
        "    tp_mask = (pred_mask == 1) & (gt_mask == 1)\n",
        "    tp_rgba = np.stack([np.zeros_like(tp_mask), np.zeros_like(tp_mask), tp_mask], axis=-1)\n",
        "    tp_rgba = np.concatenate([tp_rgba, tp_mask[..., None].astype(float)], axis=-1)\n",
        "    ax.imshow(tp_rgba, alpha=alpha_tp)\n",
        "\n",
        "    # False Alarms: prediction is 1, ground truth is 0 (green)\n",
        "    fa_mask = (pred_mask == 1) & (gt_mask == 0)\n",
        "    fa_rgba = np.stack([np.zeros_like(fa_mask), fa_mask, np.zeros_like(fa_mask)], axis=-1)\n",
        "    fa_rgba = np.concatenate([fa_rgba, fa_mask[..., None].astype(float)], axis=-1)\n",
        "    ax.imshow(fa_rgba, alpha=alpha_fa)\n",
        "\n",
        "    # Missed Alarms: prediction is 0, ground truth is 1 (magenta)\n",
        "    ma_mask = (pred_mask == 0) & (gt_mask == 1)\n",
        "    ma_rgba = np.stack([ma_mask, np.zeros_like(ma_mask), ma_mask], axis=-1)\n",
        "    ma_rgba = np.concatenate([ma_rgba, ma_mask[..., None].astype(float)], axis=-1)\n",
        "    ax.imshow(ma_rgba, alpha=alpha_ma)\n",
        "\n",
        "    # Add black border\n",
        "    ax.add_patch(Rectangle((0, 0), gray_image.shape[1], gray_image.shape[0],\n",
        "                           linewidth=border_width, edgecolor=border_color, facecolor='none'))\n",
        "\n",
        "    ax.axis('off')\n",
        "\n",
        "# === Turn Off Any Unused Axes ===\n",
        "for j in range(len(selected_indices), len(axs)):\n",
        "    axs[j].axis('off')\n",
        "\n",
        "# === Save and Show ===\n",
        "plt.subplots_adjust(wspace=0.02, hspace=0.02)\n",
        "output_path = \"./ViTSAM_Evaluation_Overlay.png\"  # <-- Update path as needed\n",
        "plt.savefig(output_path, dpi=600, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Visualization saved to: {output_path}\")\n"
      ],
      "metadata": {
        "id": "rZlIFzgTQ-pA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "from skimage.color import rgb2gray\n",
        "\n",
        "# === Configuration ===\n",
        "border_color = 'black'\n",
        "border_width = 0.35\n",
        "alpha_gray = 0.5  # transparency for base grayscale image\n",
        "alpha_tp = 0.5    # transparency for True Positives (blue)\n",
        "alpha_fa = 0.6    # False Alarms (green)\n",
        "alpha_ma = 0.6    # Missed Alarms (magenta)\n",
        "\n",
        "# === Choose which image to display ===\n",
        "index = 6  # Change this to select different image (0 to num_images-1)\n",
        "\n",
        "# === Setup Plot ===\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "\n",
        "# Convert image to grayscale\n",
        "gray_image = rgb2gray(images_tuned_list[index])\n",
        "\n",
        "# Get predicted and ground truth masks\n",
        "pred_mask = masks_tuned_list[index]\n",
        "gt_mask = ground_truth_test_masks[index]\n",
        "\n",
        "ax.imshow(gray_image, cmap='gray', interpolation='none', alpha=alpha_gray)\n",
        "\n",
        "# === Mask Overlays ===\n",
        "# True Positives: prediction and ground truth both 1 (blue)\n",
        "tp_mask = (pred_mask == 1) & (gt_mask == 1)\n",
        "tp_rgba = np.stack([np.zeros_like(tp_mask), np.zeros_like(tp_mask), tp_mask], axis=-1)\n",
        "tp_rgba = np.concatenate([tp_rgba, tp_mask[..., None].astype(float)], axis=-1)\n",
        "ax.imshow(tp_rgba, alpha=alpha_tp)\n",
        "\n",
        "# False Alarms: prediction is 1, ground truth is 0 (green)\n",
        "fa_mask = (pred_mask == 1) & (gt_mask == 0)\n",
        "fa_rgba = np.stack([np.zeros_like(fa_mask), fa_mask, np.zeros_like(fa_mask)], axis=-1)\n",
        "fa_rgba = np.concatenate([fa_rgba, fa_mask[..., None].astype(float)], axis=-1)\n",
        "ax.imshow(fa_rgba, alpha=alpha_fa)\n",
        "\n",
        "# Missed Alarms: prediction is 0, ground truth is 1 (magenta)\n",
        "ma_mask = (pred_mask == 0) & (gt_mask == 1)\n",
        "ma_rgba = np.stack([ma_mask, np.zeros_like(ma_mask), ma_mask], axis=-1)\n",
        "ma_rgba = np.concatenate([ma_rgba, ma_mask[..., None].astype(float)], axis=-1)\n",
        "ax.imshow(ma_rgba, alpha=alpha_ma)\n",
        "\n",
        "# Add black border\n",
        "ax.add_patch(Rectangle((0, 0), gray_image.shape[1], gray_image.shape[0],\n",
        "                       linewidth=border_width, edgecolor=border_color, facecolor='none'))\n",
        "ax.axis('off')\n",
        "\n",
        "# === Save and Show ===\n",
        "output_path = f\"./Evaluation_Image_{index}.png\"\n",
        "plt.savefig(output_path, dpi=600, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"Visualization saved to: {output_path}\")"
      ],
      "metadata": {
        "id": "F2kbT6GaVWak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# === Metric Computation Function ===\n",
        "def binary_segmentation_metrics(predictions, targets):\n",
        "    predictions = predictions.squeeze()\n",
        "    targets = targets.squeeze()\n",
        "    predictions_binary = (predictions > 0.5).astype(int)\n",
        "    targets_binary = targets.astype(int)\n",
        "\n",
        "    TP = np.sum((predictions_binary == 1) & (targets_binary == 1))\n",
        "    FP = np.sum((predictions_binary == 1) & (targets_binary == 0))\n",
        "    FN = np.sum((predictions_binary == 0) & (targets_binary == 1))\n",
        "    TN = np.sum((predictions_binary == 0) & (targets_binary == 0))\n",
        "\n",
        "    eps = 1e-5\n",
        "    accuracy = (TP + TN + eps) / (TP + FP + FN + TN + eps)\n",
        "    precision = (TP + eps) / (TP + FP + eps)\n",
        "    recall = (TP + eps) / (TP + FN + eps)\n",
        "    f_score = 2 * (precision * recall) / (precision + recall + eps)\n",
        "    dice = (2 * TP + eps) / (2 * TP + FP + FN + eps)\n",
        "    iou = (TP + eps) / (TP + FP + FN + eps)\n",
        "\n",
        "    total = TP + FP + FN + TN\n",
        "    p_o = (TP + TN) / total\n",
        "    p_e = ((TP + FP) * (TP + FN) + (FN + TN) * (FP + TN)) / (total ** 2)\n",
        "    kappa = (p_o - p_e) / (1 - p_e + eps)\n",
        "\n",
        "    return accuracy, precision, recall, f_score, iou, kappa, FP, FN, TP, TN, dice\n",
        "\n",
        "# === Categorization Function ===\n",
        "def categorize_metric(value, metric):\n",
        "    categories = {\n",
        "        'IoU': [(0.90, 'Excellent'), (0.85, 'Good'), (0.75, 'Fair'), (0.65, 'Poor'), (0, 'Unacceptable')],\n",
        "        'Precision': [(0.95, 'Excellent'), (0.85, 'Good'), (0.65, 'Moderate'), (0, 'Fail')],\n",
        "        'Kappa': [(0.88, 'Excellent'), (0.78, 'Good'), (0.68, 'Moderate'), (0, 'Fail')],\n",
        "        'F-Score': [(0.88, 'Excellent'), (0.78, 'Good'), (0.68, 'Moderate'), (0, 'Fail')],\n",
        "        'Recall': [(0.88, 'Excellent'), (0.78, 'Good'), (0.68, 'Moderate'), (0, 'Fail')]\n",
        "    }\n",
        "\n",
        "    if metric in categories:\n",
        "        for threshold, label in categories[metric]:\n",
        "            if value >= threshold:\n",
        "                return label\n",
        "    return \"Unknown\"\n",
        "\n",
        "# === Compute Metrics for All Images ===\n",
        "def compute_all_metrics(predictions_list, targets_list):\n",
        "    all_metrics = []\n",
        "    for i in range(len(predictions_list)):\n",
        "        metrics = binary_segmentation_metrics(predictions_list[i], targets_list[i])\n",
        "        metric_names = ['Accuracy', 'Precision', 'Recall', 'F-Score', 'IoU', 'Kappa', 'FP', 'FN', 'TP', 'TN', 'Dice']\n",
        "        metric_dict = dict(zip(metric_names, metrics))\n",
        "\n",
        "        # Add categorized versions\n",
        "        for name in ['IoU', 'Precision', 'Kappa', 'F-Score', 'Recall']:\n",
        "            metric_dict[f'{name}_Category'] = categorize_metric(metric_dict[name], name)\n",
        "\n",
        "        all_metrics.append(metric_dict)\n",
        "    return all_metrics\n",
        "\n",
        "# === Summarize Counts and Percentages ===\n",
        "def summarize_category_counts(metrics_list, metric_name, labels):\n",
        "    counts = {label: 0 for label in labels}\n",
        "    total = len(metrics_list)\n",
        "\n",
        "    for m in metrics_list:\n",
        "        category = m.get(f\"{metric_name}_Category\")\n",
        "        if category in counts:\n",
        "            counts[category] += 1\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"Counts and Percentages of Images in Each Category for {metric_name}:\")\n",
        "    for label in labels:\n",
        "        count = counts[label]\n",
        "        percentage = (count / total) * 100\n",
        "        print(f\"{label}: {count} ({percentage:.2f}%)\")\n",
        "    print()\n",
        "\n",
        "# === Run All ===\n",
        "metrics_list = compute_all_metrics(masks_tuned_list, ground_truth_test_masks)\n",
        "\n",
        "summarize_category_counts(metrics_list, 'IoU',        ['Excellent', 'Good', 'Fair', 'Poor', 'Unacceptable'])\n",
        "summarize_category_counts(metrics_list, 'Precision',  ['Excellent', 'Good', 'Moderate', 'Fail'])\n",
        "summarize_category_counts(metrics_list, 'Kappa',      ['Excellent', 'Good', 'Moderate', 'Fail'])\n",
        "summarize_category_counts(metrics_list, 'F-Score',    ['Excellent', 'Good', 'Moderate', 'Fail'])\n",
        "summarize_category_counts(metrics_list, 'Recall',     ['Excellent', 'Good', 'Moderate', 'Fail'])\n"
      ],
      "metadata": {
        "id": "0rLL093ORYTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your image\n",
        "img = cv2.imread('/content/MobileSAM/Evaluation_Image_5.png')\n",
        "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Get image dimensions\n",
        "height, width = img.shape[:2]\n",
        "print(f\"Image dimensions: width={width}, height={height}\")\n",
        "\n",
        "# Display the image to see coordinates\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(img_rgb)\n",
        "plt.title('Click to see coordinates')\n",
        "plt.axis('on')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5Q2J03wfYWaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Load image\n",
        "img = cv2.imread('/content/MobileSAM/Evaluation_Image_6.png')\n",
        "\n",
        "left_roi_pts = np.array([\n",
        "    [550, 2500],    # top-left\n",
        "    [850, 2500],    # top-right\n",
        "    [900, 2900],    # bottom-right\n",
        "    [600, 2900]     # bottom-left\n",
        "], np.int32)\n",
        "\n",
        "# Right ROI - same 300 pixel width as left\n",
        "right_roi_pts = np.array([\n",
        "    [3150, 2500],   # top-left (moved 50 left)\n",
        "    [3450, 2500],   # top-right (moved 50 right)\n",
        "    [3400, 2900],   # bottom-right (moved 50 right)\n",
        "    [3100, 2900]    # bottom-left (moved 50 left)\n",
        "], np.int32)\n",
        "\n",
        "# Visualize both ROIs on the original image\n",
        "img_with_rois = img.copy()\n",
        "\n",
        "# Draw tilted rectangles (polygons)\n",
        "cv2.polylines(img_with_rois, [left_roi_pts], True, (0, 255, 0), 2)\n",
        "cv2.polylines(img_with_rois, [right_roi_pts], True, (0, 0, 255), 2)\n",
        "\n",
        "# Add text labels\n",
        "cv2.putText(img_with_rois, \"LEFT\", (600, 2390), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
        "cv2.putText(img_with_rois, \"RIGHT\", (3200, 2390), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
        "\n",
        "# Display in Colab\n",
        "cv2_imshow(img_with_rois)\n",
        "\n",
        "# Extract ROI regions using masks\n",
        "# Create mask for left ROI\n",
        "mask_left = np.zeros(img.shape[:2], dtype=np.uint8)\n",
        "cv2.fillPoly(mask_left, [left_roi_pts], 255)\n",
        "left_bank_roi = cv2.bitwise_and(img, img, mask=mask_left)\n",
        "\n",
        "# Create mask for right ROI\n",
        "mask_right = np.zeros(img.shape[:2], dtype=np.uint8)\n",
        "cv2.fillPoly(mask_right, [right_roi_pts], 255)\n",
        "right_bank_roi = cv2.bitwise_and(img, img, mask=mask_right)\n",
        "\n",
        "print(f\"Left Bank ROI polygon: {left_roi_pts.tolist()}\")\n",
        "print(f\"Right Bank ROI polygon: {right_roi_pts.tolist()}\")\n",
        "\n",
        "# Save\n",
        "cv2.imwrite('both_rois_marked_tilted.png', img_with_rois)\n",
        "\n",
        "# Get bounding rectangles for cropped ROIs\n",
        "x_l, y_l, w_l, h_l = cv2.boundingRect(left_roi_pts)\n",
        "x_r, y_r, w_r, h_r = cv2.boundingRect(right_roi_pts)\n",
        "\n",
        "left_bank_cropped = img[y_l:y_l+h_l, x_l:x_l+w_l]\n",
        "right_bank_cropped = img[y_r:y_r+h_r, x_r:x_r+w_r]\n",
        "\n",
        "cv2.imwrite('left_bank_roi_tilted.png', left_bank_cropped)\n",
        "cv2.imwrite('right_bank_roi_tilted.png', right_bank_cropped)\n",
        "\n",
        "# Display the cropped ROIs\n",
        "print(\"\\nLeft Bank ROI (cropped):\")\n",
        "cv2_imshow(left_bank_cropped)\n",
        "\n",
        "print(\"\\nRight Bank ROI (cropped):\")\n",
        "cv2_imshow(right_bank_cropped)"
      ],
      "metadata": {
        "id": "RU4h0JN8T5Uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Read the image\n",
        "img = cv2.imread('both_rois_marked_tilted.png')\n",
        "\n",
        "# Define ROIs\n",
        "left_roi_pts = np.array([\n",
        "    [550, 2500],\n",
        "    [850, 2500],\n",
        "    [900, 2900],\n",
        "    [600, 2900]\n",
        "], np.int32)\n",
        "\n",
        "right_roi_pts = np.array([\n",
        "    [3150, 2500],\n",
        "    [3450, 2500],\n",
        "    [3400, 2900],\n",
        "    [3100, 2900]\n",
        "], np.int32)\n",
        "\n",
        "# Create masks for each ROI\n",
        "left_mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
        "right_mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
        "cv2.fillPoly(left_mask, [left_roi_pts], 255)\n",
        "cv2.fillPoly(right_mask, [right_roi_pts], 255)\n",
        "\n",
        "# Define blue color range (water pixels)\n",
        "lower_blue = np.array([100, 0, 0])\n",
        "upper_blue = np.array([255, 100, 100])\n",
        "\n",
        "# Find blue pixels\n",
        "blue_mask = cv2.inRange(img, lower_blue, upper_blue)\n",
        "\n",
        "# Count water pixels in each ROI\n",
        "left_water_pixels = cv2.countNonZero(cv2.bitwise_and(blue_mask, left_mask))\n",
        "right_water_pixels = cv2.countNonZero(cv2.bitwise_and(blue_mask, right_mask))\n",
        "\n",
        "print(f\"Left ROI water pixels: {left_water_pixels}\")\n",
        "print(f\"Right ROI water pixels: {right_water_pixels}\")"
      ],
      "metadata": {
        "id": "I91cQytwhJBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Load image\n",
        "img = cv2.imread('/content/MobileSAM/Evaluation_Image_5.png')\n",
        "\n",
        "left_roi_pts = np.array([\n",
        "    [550, 2500],    # top-left\n",
        "    [850, 2500],    # top-right\n",
        "    [900, 2900],    # bottom-right\n",
        "    [600, 2900]     # bottom-left\n",
        "], np.int32)\n",
        "\n",
        "# Right ROI - same 300 pixel width as left\n",
        "right_roi_pts = np.array([\n",
        "    [3150, 2500],   # top-left (moved 50 left)\n",
        "    [3450, 2500],   # top-right (moved 50 right)\n",
        "    [3400, 2900],   # bottom-right (moved 50 right)\n",
        "    [3100, 2900]    # bottom-left (moved 50 left)\n",
        "], np.int32)\n",
        "\n",
        "# Visualize both ROIs on the original image\n",
        "img_with_rois = img.copy()\n",
        "\n",
        "# Draw tilted rectangles (polygons)\n",
        "cv2.polylines(img_with_rois, [left_roi_pts], True, (0, 255, 0), 2)\n",
        "cv2.polylines(img_with_rois, [right_roi_pts], True, (0, 0, 255), 2)\n",
        "\n",
        "# Add text labels\n",
        "cv2.putText(img_with_rois, \"LEFT\", (600, 2390), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
        "cv2.putText(img_with_rois, \"RIGHT\", (3200, 2390), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
        "\n",
        "# Display in Colab\n",
        "cv2_imshow(img_with_rois)\n",
        "\n",
        "# Extract ROI regions using masks\n",
        "# Create mask for left ROI\n",
        "mask_left = np.zeros(img.shape[:2], dtype=np.uint8)\n",
        "cv2.fillPoly(mask_left, [left_roi_pts], 255)\n",
        "left_bank_roi = cv2.bitwise_and(img, img, mask=mask_left)\n",
        "\n",
        "# Create mask for right ROI\n",
        "mask_right = np.zeros(img.shape[:2], dtype=np.uint8)\n",
        "cv2.fillPoly(mask_right, [right_roi_pts], 255)\n",
        "right_bank_roi = cv2.bitwise_and(img, img, mask=mask_right)\n",
        "\n",
        "print(f\"Left Bank ROI polygon: {left_roi_pts.tolist()}\")\n",
        "print(f\"Right Bank ROI polygon: {right_roi_pts.tolist()}\")\n",
        "\n",
        "# Save\n",
        "cv2.imwrite('both_rois_marked_tilted.png', img_with_rois)\n",
        "\n",
        "# Get bounding rectangles for cropped ROIs\n",
        "x_l, y_l, w_l, h_l = cv2.boundingRect(left_roi_pts)\n",
        "x_r, y_r, w_r, h_r = cv2.boundingRect(right_roi_pts)\n",
        "\n",
        "left_bank_cropped = img[y_l:y_l+h_l, x_l:x_l+w_l]\n",
        "right_bank_cropped = img[y_r:y_r+h_r, x_r:x_r+w_r]\n",
        "\n",
        "cv2.imwrite('left_bank_roi_tilted.png', left_bank_cropped)\n",
        "cv2.imwrite('right_bank_roi_tilted.png', right_bank_cropped)\n",
        "\n",
        "# Display the cropped ROIs\n",
        "print(\"\\nLeft Bank ROI (cropped):\")\n",
        "cv2_imshow(left_bank_cropped)\n",
        "\n",
        "print(\"\\nRight Bank ROI (cropped):\")\n",
        "cv2_imshow(right_bank_cropped)"
      ],
      "metadata": {
        "id": "Hmtz3EsVialW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Read the image\n",
        "img = cv2.imread('both_rois_marked_tilted.png')\n",
        "\n",
        "# Define ROIs\n",
        "left_roi_pts = np.array([\n",
        "    [550, 2500],\n",
        "    [850, 2500],\n",
        "    [900, 2900],\n",
        "    [600, 2900]\n",
        "], np.int32)\n",
        "\n",
        "right_roi_pts = np.array([\n",
        "    [3150, 2500],\n",
        "    [3450, 2500],\n",
        "    [3400, 2900],\n",
        "    [3100, 2900]\n",
        "], np.int32)\n",
        "\n",
        "# Create masks for each ROI\n",
        "left_mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
        "right_mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
        "cv2.fillPoly(left_mask, [left_roi_pts], 255)\n",
        "cv2.fillPoly(right_mask, [right_roi_pts], 255)\n",
        "\n",
        "# Define blue color range (water pixels)\n",
        "lower_blue = np.array([100, 0, 0])\n",
        "upper_blue = np.array([255, 100, 100])\n",
        "\n",
        "# Find blue pixels\n",
        "blue_mask = cv2.inRange(img, lower_blue, upper_blue)\n",
        "\n",
        "# Count water pixels in each ROI\n",
        "left_water_pixels = cv2.countNonZero(cv2.bitwise_and(blue_mask, left_mask))\n",
        "right_water_pixels = cv2.countNonZero(cv2.bitwise_and(blue_mask, right_mask))\n",
        "\n",
        "print(f\"Left ROI water pixels: {left_water_pixels}\")\n",
        "print(f\"Right ROI water pixels: {right_water_pixels}\")"
      ],
      "metadata": {
        "id": "sJpvt28zioi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Load image\n",
        "img = cv2.imread('/content/MobileSAM/Evaluation_Image_10.png')\n",
        "\n",
        "left_roi_pts = np.array([\n",
        "    [550, 2500],    # top-left\n",
        "    [850, 2500],    # top-right\n",
        "    [900, 2900],    # bottom-right\n",
        "    [600, 2900]     # bottom-left\n",
        "], np.int32)\n",
        "\n",
        "# Right ROI - same 300 pixel width as left\n",
        "right_roi_pts = np.array([\n",
        "    [3150, 2500],   # top-left (moved 50 left)\n",
        "    [3450, 2500],   # top-right (moved 50 right)\n",
        "    [3400, 2900],   # bottom-right (moved 50 right)\n",
        "    [3100, 2900]    # bottom-left (moved 50 left)\n",
        "], np.int32)\n",
        "\n",
        "# Visualize both ROIs on the original image\n",
        "img_with_rois = img.copy()\n",
        "\n",
        "# Draw tilted rectangles (polygons)\n",
        "cv2.polylines(img_with_rois, [left_roi_pts], True, (0, 255, 0), 2)\n",
        "cv2.polylines(img_with_rois, [right_roi_pts], True, (0, 0, 255), 2)\n",
        "\n",
        "# Add text labels\n",
        "cv2.putText(img_with_rois, \"LEFT\", (600, 2390), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
        "cv2.putText(img_with_rois, \"RIGHT\", (3200, 2390), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
        "\n",
        "# Display in Colab\n",
        "cv2_imshow(img_with_rois)\n",
        "\n",
        "# Extract ROI regions using masks\n",
        "# Create mask for left ROI\n",
        "mask_left = np.zeros(img.shape[:2], dtype=np.uint8)\n",
        "cv2.fillPoly(mask_left, [left_roi_pts], 255)\n",
        "left_bank_roi = cv2.bitwise_and(img, img, mask=mask_left)\n",
        "\n",
        "# Create mask for right ROI\n",
        "mask_right = np.zeros(img.shape[:2], dtype=np.uint8)\n",
        "cv2.fillPoly(mask_right, [right_roi_pts], 255)\n",
        "right_bank_roi = cv2.bitwise_and(img, img, mask=mask_right)\n",
        "\n",
        "print(f\"Left Bank ROI polygon: {left_roi_pts.tolist()}\")\n",
        "print(f\"Right Bank ROI polygon: {right_roi_pts.tolist()}\")\n",
        "\n",
        "# Save\n",
        "cv2.imwrite('both_rois_marked_tilted.png', img_with_rois)\n",
        "\n",
        "# Get bounding rectangles for cropped ROIs\n",
        "x_l, y_l, w_l, h_l = cv2.boundingRect(left_roi_pts)\n",
        "x_r, y_r, w_r, h_r = cv2.boundingRect(right_roi_pts)\n",
        "\n",
        "left_bank_cropped = img[y_l:y_l+h_l, x_l:x_l+w_l]\n",
        "right_bank_cropped = img[y_r:y_r+h_r, x_r:x_r+w_r]\n",
        "\n",
        "cv2.imwrite('left_bank_roi_tilted.png', left_bank_cropped)\n",
        "cv2.imwrite('right_bank_roi_tilted.png', right_bank_cropped)\n",
        "\n",
        "# Display the cropped ROIs\n",
        "print(\"\\nLeft Bank ROI (cropped):\")\n",
        "cv2_imshow(left_bank_cropped)\n",
        "\n",
        "print(\"\\nRight Bank ROI (cropped):\")\n",
        "cv2_imshow(right_bank_cropped)"
      ],
      "metadata": {
        "id": "CLRABDMwkXeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Read the image\n",
        "img = cv2.imread('both_rois_marked_tilted.png')\n",
        "\n",
        "# Define ROIs\n",
        "left_roi_pts = np.array([\n",
        "    [550, 2500],\n",
        "    [850, 2500],\n",
        "    [900, 2900],\n",
        "    [600, 2900]\n",
        "], np.int32)\n",
        "\n",
        "right_roi_pts = np.array([\n",
        "    [3150, 2500],\n",
        "    [3450, 2500],\n",
        "    [3400, 2900],\n",
        "    [3100, 2900]\n",
        "], np.int32)\n",
        "\n",
        "# Create masks for each ROI\n",
        "left_mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
        "right_mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
        "cv2.fillPoly(left_mask, [left_roi_pts], 255)\n",
        "cv2.fillPoly(right_mask, [right_roi_pts], 255)\n",
        "\n",
        "# Define blue color range (water pixels)\n",
        "lower_blue = np.array([100, 0, 0])\n",
        "upper_blue = np.array([255, 100, 100])\n",
        "\n",
        "# Find blue pixels\n",
        "blue_mask = cv2.inRange(img, lower_blue, upper_blue)\n",
        "\n",
        "# Count water pixels in each ROI\n",
        "left_water_pixels = cv2.countNonZero(cv2.bitwise_and(blue_mask, left_mask))\n",
        "right_water_pixels = cv2.countNonZero(cv2.bitwise_and(blue_mask, right_mask))\n",
        "\n",
        "print(f\"Left ROI water pixels: {left_water_pixels}\")\n",
        "print(f\"Right ROI water pixels: {right_water_pixels}\")"
      ],
      "metadata": {
        "id": "E6gEn8Lokuvc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}