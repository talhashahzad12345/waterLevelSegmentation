{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eq5vfzYtTfDq"
      },
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "from numpy import zeros\n",
        "from numpy.random import randint\n",
        "import torch\n",
        "import os\n",
        "import cv2\n",
        "from statistics import mean\n",
        "from torch.nn.functional import threshold, normalize\n",
        "\n",
        "# Data Viz\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torch torchvision &> /dev/null\n",
        "! pip install opencv-python pycocotools matplotlib onnxruntime onnx &> /dev/null\n",
        "\n",
        "# ==== Download Pretrained SAM Model Weights ====\n",
        "# Download the ViT versions of the SAM model weights from Facebook's public storage.\n",
        "! pip install git+https://github.com/facebookresearch/segment-anything.git &> /dev/null\n",
        "! wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth &> /dev/null\n",
        "! wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth &> /dev/null\n",
        "! wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth &> /dev/null"
      ],
      "metadata": {
        "id": "5lcxJ4PPTnVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === CONFIGURATION ===\n",
        "# Set the path to your training images and labels\n",
        "image_path = \"/content/drive/MyDrive/canal_project/27_11_2025/train/images\"   # <-- Update this path\n",
        "label_path = \"/content/drive/MyDrive/canal_project/27_11_2025/train/masks\"    # <-- Update this path\n",
        "# === Load Image Paths ===\n",
        "# Count total number of image files (e.g., .jpg format)\n",
        "all_image_paths = sorted(glob(os.path.join(image_path, \"*.jpg\")))  # Use .png if needed\n",
        "total_images = len(all_image_paths)\n",
        "print(f\"Total Number of Images: {total_images}\")\n",
        "\n",
        "# === Load Label Paths ===\n",
        "# Count total number of label files (e.g., .png format for segmentation masks)\n",
        "all_label_paths = sorted(glob(os.path.join(label_path, \"*.png\")))\n",
        "total_labels = len(all_label_paths)\n",
        "print(f\"Total Number of Labels: {total_labels}\")\n",
        "\n",
        "# === Match Images and Labels ===\n",
        "# Assuming both are in matching order and of equal length\n",
        "train_image_paths = all_image_paths[:total_images]\n",
        "train_label_paths = all_label_paths[:total_labels]\n",
        "\n",
        "# Preview label paths (for verification)\n",
        "print(\"Sample label paths:\")\n",
        "for path in train_label_paths[:5]:\n",
        "    print(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2g6w85uTuVl",
        "outputId": "d64e861d-69e5-4a7e-c4b7-911a11e90974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Number of Images: 186\n",
            "Total Number of Labels: 186\n",
            "Sample label paths:\n",
            "/content/drive/MyDrive/canal_project/27_11_2025/train/masks/image_capture_2025-04-05_11-45-56_jpg.rf.3d31effc5cd44f2feb5ca1c2c219346b_mask.png\n",
            "/content/drive/MyDrive/canal_project/27_11_2025/train/masks/image_capture_2025-04-06_10-10-41_jpg.rf.2d25833c6629824eb13665eb4b7a794c_mask.png\n",
            "/content/drive/MyDrive/canal_project/27_11_2025/train/masks/image_capture_2025-04-06_11-10-42_jpg.rf.ddc1a4d0ad9d75799e5114aa78022e19_mask.png\n",
            "/content/drive/MyDrive/canal_project/27_11_2025/train/masks/image_capture_2025-04-06_11-11-48_jpg.rf.6b1a6629a791c2c0c47e5ed49cca16ee_mask.png\n",
            "/content/drive/MyDrive/canal_project/27_11_2025/train/masks/image_capture_2025-04-06_12-12-56_jpg.rf.a087e1490f708cdb45e28555f3786aaf_mask.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === CONFIGURATION ===\n",
        "# Set the path to your validation images and labels\n",
        "val_image_path = \"/content/drive/MyDrive/canal_project/27_11_2025/valid/images\"    # <-- Update this path\n",
        "val_label_path = \"/content/drive/MyDrive/canal_project/27_11_2025/valid/masks\"      # <-- Update this path\n",
        "\n",
        "# === Load Validation Image Paths ===\n",
        "# Collect and sort all .jpg image files in the validation folder\n",
        "val_all_image_paths = sorted(glob(os.path.join(val_image_path, \"*.jpg\")))\n",
        "val_total_images = len(val_all_image_paths)\n",
        "print(f\"Total Number of Validation Images: {val_total_images}\")\n",
        "\n",
        "# === Load Validation Label Paths ===\n",
        "# Collect and sort all .png label files in the validation folder\n",
        "val_all_label_paths = sorted(glob(os.path.join(val_label_path, \"*.png\")))\n",
        "val_total_labels = len(val_all_label_paths)\n",
        "print(f\"Total Number of Validation Labels: {val_total_labels}\")\n",
        "\n",
        "# === Match Images and Labels (by order) ===\n",
        "# This assumes one-to-one correspondence between image and label files\n",
        "Val1_image_paths = val_all_image_paths[:val_total_images]\n",
        "Val1_label_paths = val_all_label_paths[:val_total_labels]\n",
        "\n",
        "# Preview a few label paths to confirm loading\n",
        "print(\"Sample validation label paths:\")\n",
        "for path in Val1_label_paths[:5]:\n",
        "    print(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XWuqC4UT8IF",
        "outputId": "2a45e597-97c1-42bd-f0de-ca9fbe059150"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Number of Validation Images: 39\n",
            "Total Number of Validation Labels: 39\n",
            "Sample validation label paths:\n",
            "/content/drive/MyDrive/canal_project/27_11_2025/valid/masks/image_capture_2025-04-05_11-44-49_jpg.rf.b321be12d6ab3078ae5064300cfb22ac_mask.png\n",
            "/content/drive/MyDrive/canal_project/27_11_2025/valid/masks/image_capture_2025-04-07_11-37-39_jpg.rf.2fabed3881bc90ecc487ee025e0509cb_mask.png\n",
            "/content/drive/MyDrive/canal_project/27_11_2025/valid/masks/image_capture_2025-04-08_11-04-35_jpg.rf.da8fbcd31f962ae95b3d5301f544de7e_mask.png\n",
            "/content/drive/MyDrive/canal_project/27_11_2025/valid/masks/image_capture_2025-04-09_11-31-33_jpg.rf.f831b52a22d28ffa7e8e351d3e28381b_mask.png\n",
            "/content/drive/MyDrive/canal_project/27_11_2025/valid/masks/image_capture_2025-05-19_12-47-22_jpg.rf.7deac398fbf7675b3cb73df0620b94d2_mask.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Please dont run this line if you would like to use the original size of input images.\n",
        "desired_size=(640, 640)"
      ],
      "metadata": {
        "id": "S39Ms-aGUEAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Load and Process Ground Truth Masks ===\n",
        "# This dictionary will store binary masks where pixel > 0 is treated as True\n",
        "ground_truth_masks = {}\n",
        "\n",
        "for idx in range(len(train_label_paths)):\n",
        "    # Read the label mask in grayscale\n",
        "    gt_grayscale = cv2.imread(train_label_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Resize the mask if desired_size is specified\n",
        "    if desired_size is not None:\n",
        "        gt_grayscale = cv2.resize(gt_grayscale, desired_size, interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    # Convert to binary mask (True where pixel > 0)\n",
        "    ground_truth_masks[idx] = (gt_grayscale > 0)\n",
        "\n",
        "# Optional: Print number of masks and preview a sample\n",
        "print(f\"Total ground truth masks loaded: {len(ground_truth_masks)}\")\n",
        "print(\"Example binary mask shape:\", ground_truth_masks[0].shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoLvPnpTUJ9M",
        "outputId": "72e6dc31-5f76-4cb3-bb43-8a78dc8fe06e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total ground truth masks loaded: 186\n",
            "Example binary mask shape: (640, 640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Load and Process Validation Ground Truth Masks ===\n",
        "# This dictionary will store binary masks for validation data\n",
        "ground_truth_masksv = {}\n",
        "\n",
        "for idx in range(len(Val1_label_paths)):\n",
        "    # Read the validation label mask in grayscale\n",
        "    gt_grayscale = cv2.imread(Val1_label_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Resize the mask if a desired size is specified\n",
        "    if desired_size is not None:\n",
        "        gt_grayscale = cv2.resize(gt_grayscale, desired_size, interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    # Convert to binary mask: True where pixel > 0\n",
        "    ground_truth_masksv[idx] = (gt_grayscale > 0)\n",
        "\n",
        "# Print summary\n",
        "print(f\"Total validation ground truth masks loaded: {len(ground_truth_masksv)}\")\n",
        "print(\"Example validation mask shape:\", ground_truth_masksv[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1wbJXEHUMxt",
        "outputId": "623a84a9-5392-4288-850a-7e9eac0b50e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total validation ground truth masks loaded: 39\n",
            "Example validation mask shape: (640, 640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_type = 'vit_b'\n",
        "checkpoint = 'sam_vit_b_01ec64.pth'\n",
        "device = 'cuda:0'"
      ],
      "metadata": {
        "id": "4g4PKrszUNyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_type = 'vit_l'\n",
        "checkpoint = 'sam_vit_l_0b3195.pth'\n",
        "device = 'cuda:0'"
      ],
      "metadata": {
        "id": "qAAxqfkAUS-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_type = \"vit_h\"\n",
        "checkpoint = \"sam_vit_h_4b8939.pth\"\n",
        "device = 'cuda:0'"
      ],
      "metadata": {
        "id": "vxeUakqRUU-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Import Required SAM Modules ===\n",
        "# Make sure the Segment Anything (SAM) package is installed and accessible\n",
        "from segment_anything import SamPredictor, sam_model_registry\n",
        "import torch\n",
        "\n",
        "# === Configuration ===\n",
        "# Set the model type: \"vit_b\", \"vit_l\", or \"vit_h\" depending on your .pth file\n",
        "model_type = \"vit_b\"  # or \"vit_l\", \"vit_h\", etc.\n",
        "\n",
        "# Path to the pretrained SAM checkpoint file (.pth)\n",
        "checkpoint = \"/content/sam_vit_b_01ec64.pth\"  # <-- Update with actual path\n",
        "\n",
        "# === Load Model ===\n",
        "# Use the model registry to initialize the correct SAM architecture\n",
        "sam_model = sam_model_registry[model_type](checkpoint=checkpoint)\n",
        "\n",
        "# Move model to GPU if available, otherwise CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "sam_model.to(device)\n",
        "\n",
        "# === Set Model to Training Mode ===\n",
        "# Use `model.train()` when fine-tuning or training the model\n",
        "# For inference, use `model.eval()` instead\n",
        "sam_model.train()\n",
        "\n",
        "print(f\"SAM model ({model_type}) loaded on {device} and set to training mode.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-zohZg4UaRN",
        "outputId": "be708769-7253-45e1-d6d8-3019af1996cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SAM model (vit_b) loaded on cuda and set to training mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from segment_anything.utils.transforms import ResizeLongestSide\n",
        "\n",
        "# Preprocessed image data will be stored in this dictionary\n",
        "transformed_data = defaultdict(dict)\n",
        "\n",
        "# Transformer that resizes image while preserving aspect ratio\n",
        "resize_transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
        "\n",
        "# === Image Preprocessing Loop ===\n",
        "for idx in range(len(train_image_paths)):\n",
        "    # Load image from path\n",
        "    image = cv2.imread(train_image_paths[idx])\n",
        "\n",
        "    # Resize if a fixed input size is specified (e.g., for training consistency)\n",
        "    if desired_size is not None:\n",
        "        image = cv2.resize(image, desired_size, interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    # Convert BGR (OpenCV default) to RGB (SAM model expects RGB)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Apply SAM’s resizing transformation to match its input constraints\n",
        "    input_image_np = resize_transform.apply_image(image_rgb)\n",
        "\n",
        "    # Convert NumPy array to torch tensor and add batch dimension\n",
        "    input_image_tensor = torch.as_tensor(input_image_np, device=device)\n",
        "    input_image_tensor = input_image_tensor.permute(2, 0, 1).contiguous()[None, :, :, :]  # Shape: [1, 3, H, W]\n",
        "\n",
        "    # Preprocess using SAM model’s preprocessing method (normalization, padding, etc.)\n",
        "    input_tensor = sam_model.preprocess(input_image_tensor)\n",
        "\n",
        "    # Store processed data\n",
        "    transformed_data[idx]['image'] = input_tensor                          # Preprocessed image tensor\n",
        "    transformed_data[idx]['input_size'] = input_image_tensor.shape[-2:]   # Input tensor size (H, W)\n",
        "    transformed_data[idx]['original_image_size'] = image_rgb.shape[:2]    # Original image size (H, W)\n",
        "\n",
        "print(f\"Processed {len(transformed_data)} training images for SAM input.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AmxgtHiUfRW",
        "outputId": "300d5c71-6134-419a-c8bf-6acf94bd29f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 186 training images for SAM input.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Training Hyperparameters ===\n",
        "lr = 1e-5                     # Learning rate for optimizer\n",
        "wd = 0                        # Weight decay (L2 regularization)\n",
        "batch_size = 32              # Number of samples per batch\n",
        "num_epochs = 5               # Total number of training epochs\n",
        "\n",
        "# === Optimizer Setup ===\n",
        "# Only the mask decoder parameters are being fine-tuned (others are frozen)\n",
        "optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "# === Loss Function ===\n",
        "# Binary Cross Entropy with logits is commonly used for binary segmentation\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "# === Device Setup ===\n",
        "# Automatically use GPU if available, otherwise fallback to CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# === Ground Truth Mask Keys ===\n",
        "# These lists are used to index into your ground truth dictionaries\n",
        "keys_train = list(ground_truth_masks.keys())\n",
        "keys_valid = list(ground_truth_masksv.keys())\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"Training on {len(keys_train)} images, validating on {len(keys_valid)} images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wS-fIg0UUkwE",
        "outputId": "687342ef-8cda-4538-8166-2599639afa53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Training on 186 images, validating on 39 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# === Validation DataLoader Setup ===\n",
        "# Here we're using a list of file paths as the dataset, which will later need to be wrapped in a proper Dataset class\n",
        "val_loader = DataLoader(Val1_image_paths, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# === Basic Validation Dataset Checks ===\n",
        "\n",
        "# Total number of validation examples\n",
        "num_val_examples = len(Val1_image_paths)\n",
        "print(f\"Number of validation examples: {num_val_examples}\")\n",
        "\n",
        "# Number of items returned by val_loader.dataset (same as above since it's a list)\n",
        "print(f\"Number of examples in validation dataset (via DataLoader): {len(val_loader.dataset)}\")\n",
        "\n",
        "# Number of batches in the validation DataLoader\n",
        "print(f\"Number of batches in validation loader: {len(val_loader)}\")\n",
        "\n",
        "# === Safety Check ===\n",
        "# Prevent training from continuing if validation data is empty\n",
        "if num_val_examples == 0:\n",
        "    raise ValueError(\"The validation dataset is empty. Please check your data paths.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGkkf6cpUnOm",
        "outputId": "773e27aa-b8a4-4f18-f673-1ec3965ac1e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of validation examples: 39\n",
            "Number of examples in validation dataset (via DataLoader): 39\n",
            "Number of batches in validation loader: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "gR1WLPhvUvm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Accuracy Calculation Function ===\n",
        "def calculate_accuracy(predictions, targets):\n",
        "    \"\"\"\n",
        "    Computes binary accuracy between predicted and ground truth masks.\n",
        "    \"\"\"\n",
        "    binary_predictions = (predictions > 0.5).float()\n",
        "    accuracy = (binary_predictions == targets).float().mean()\n",
        "    return accuracy.item()\n",
        "\n",
        "# === Batch Training Function ===\n",
        "def train_on_batch(keys, batch_start, batch_end):\n",
        "    \"\"\"\n",
        "    Trains the SAM mask decoder on a batch of images and masks.\n",
        "    Returns batch loss and accuracy.\n",
        "    \"\"\"\n",
        "    batch_losses = []\n",
        "    batch_accuracies = []\n",
        "\n",
        "    for k in keys[batch_start:batch_end]:\n",
        "        # === Get input data and metadata\n",
        "        input_image = transformed_data[k]['image'].to(device)\n",
        "        input_size = transformed_data[k]['input_size']\n",
        "        original_image_size = transformed_data[k]['original_image_size']\n",
        "\n",
        "        # === Forward Pass (frozen encoders)\n",
        "        with torch.no_grad():\n",
        "            image_embedding = sam_model.image_encoder(input_image)\n",
        "            sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n",
        "                points=None, boxes=None, masks=None\n",
        "            )\n",
        "\n",
        "        low_res_masks, _ = sam_model.mask_decoder(\n",
        "            image_embeddings=image_embedding,\n",
        "            image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
        "            sparse_prompt_embeddings=sparse_embeddings,\n",
        "            dense_prompt_embeddings=dense_embeddings,\n",
        "            multimask_output=False\n",
        "        )\n",
        "\n",
        "        # === Resize prediction to original size\n",
        "        upscaled_masks = sam_model.postprocess_masks(\n",
        "            low_res_masks, input_size, original_image_size\n",
        "        ).to(device)\n",
        "\n",
        "        # === Resize ground truth mask to match output\n",
        "        gt_np = ground_truth_masks[k].astype(np.uint8)\n",
        "        resized_gt = cv2.resize(gt_np, upscaled_masks.shape[-2:][::-1], interpolation=cv2.INTER_NEAREST)\n",
        "        gt_binary_mask = torch.tensor(resized_gt, dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "        # === Compute loss and update weights\n",
        "        loss = loss_fn(upscaled_masks, gt_binary_mask)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # === Record metrics\n",
        "        batch_losses.append(loss.item())\n",
        "        batch_accuracies.append(calculate_accuracy(torch.sigmoid(upscaled_masks), gt_binary_mask))\n",
        "\n",
        "    return batch_losses, batch_accuracies\n",
        "\n",
        "# === Epoch Training Loop ===\n",
        "losses, accuracies = [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_losses = []\n",
        "    epoch_accuracies = []\n",
        "\n",
        "    print(f\"\\n--- EPOCH {epoch + 1}/{num_epochs} ---\")\n",
        "\n",
        "    for batch_start in range(0, len(keys), batch_size):\n",
        "        batch_end = min(batch_start + batch_size, len(keys))\n",
        "        batch_losses, batch_accuracies = train_on_batch(keys, batch_start, batch_end)\n",
        "\n",
        "        batch_loss = mean(batch_losses)\n",
        "        batch_accuracy = mean(batch_accuracies)\n",
        "        epoch_losses.append(batch_loss)\n",
        "        epoch_accuracies.extend(batch_accuracies)\n",
        "\n",
        "        print(f'Batch [{batch_start + 1}–{batch_end}] | Loss: {batch_loss:.6f} | Accuracy: {batch_accuracy:.4f}')\n",
        "\n",
        "    # === End of Epoch ===\n",
        "    mean_train_loss = mean(epoch_losses)\n",
        "    mean_train_accuracy = mean(epoch_accuracies)\n",
        "    losses.append(mean_train_loss)\n",
        "    accuracies.append(mean_train_accuracy)\n",
        "\n",
        "    print(f'\\nEpoch {epoch + 1} Summary:')\n",
        "    print(f'➤ Mean Training Loss: {mean_train_loss:.6f}')\n",
        "    print(f'➤ Mean Training Accuracy: {mean_train_accuracy:.4f}')\n",
        "\n",
        "    # Clear cache to manage memory\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bv5YxtqwU0xw",
        "outputId": "2acf357e-c143-4895-fc19-692562adfd5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- EPOCH 1/5 ---\n",
            "Batch [1–32] | Loss: 0.983773 | Accuracy: 0.9191\n",
            "Batch [33–64] | Loss: 0.037950 | Accuracy: 0.9905\n",
            "Batch [65–96] | Loss: 0.042169 | Accuracy: 0.9853\n",
            "Batch [97–128] | Loss: 0.031729 | Accuracy: 0.9930\n",
            "Batch [129–160] | Loss: 0.026552 | Accuracy: 0.9918\n",
            "Batch [161–186] | Loss: 0.015162 | Accuracy: 0.9956\n",
            "\n",
            "Epoch 1 Summary:\n",
            "➤ Mean Training Loss: 0.189556\n",
            "➤ Mean Training Accuracy: 0.9787\n",
            "\n",
            "--- EPOCH 2/5 ---\n",
            "Batch [1–32] | Loss: 0.015650 | Accuracy: 0.9955\n",
            "Batch [33–64] | Loss: 0.014300 | Accuracy: 0.9957\n",
            "Batch [65–96] | Loss: 0.017627 | Accuracy: 0.9941\n",
            "Batch [97–128] | Loss: 0.014464 | Accuracy: 0.9953\n",
            "Batch [129–160] | Loss: 0.019018 | Accuracy: 0.9925\n",
            "Batch [161–186] | Loss: 0.012470 | Accuracy: 0.9957\n",
            "\n",
            "Epoch 2 Summary:\n",
            "➤ Mean Training Loss: 0.015588\n",
            "➤ Mean Training Accuracy: 0.9948\n",
            "\n",
            "--- EPOCH 3/5 ---\n",
            "Batch [1–32] | Loss: 0.012553 | Accuracy: 0.9959\n",
            "Batch [33–64] | Loss: 0.011836 | Accuracy: 0.9959\n",
            "Batch [65–96] | Loss: 0.014567 | Accuracy: 0.9948\n",
            "Batch [97–128] | Loss: 0.012936 | Accuracy: 0.9955\n",
            "Batch [129–160] | Loss: 0.015756 | Accuracy: 0.9938\n",
            "Batch [161–186] | Loss: 0.011143 | Accuracy: 0.9959\n",
            "\n",
            "Epoch 3 Summary:\n",
            "➤ Mean Training Loss: 0.013132\n",
            "➤ Mean Training Accuracy: 0.9953\n",
            "\n",
            "--- EPOCH 4/5 ---\n",
            "Batch [1–32] | Loss: 0.010997 | Accuracy: 0.9960\n",
            "Batch [33–64] | Loss: 0.010372 | Accuracy: 0.9961\n",
            "Batch [65–96] | Loss: 0.012614 | Accuracy: 0.9951\n",
            "Batch [97–128] | Loss: 0.011969 | Accuracy: 0.9956\n",
            "Batch [129–160] | Loss: 0.014015 | Accuracy: 0.9946\n",
            "Batch [161–186] | Loss: 0.010243 | Accuracy: 0.9961\n",
            "\n",
            "Epoch 4 Summary:\n",
            "➤ Mean Training Loss: 0.011702\n",
            "➤ Mean Training Accuracy: 0.9956\n",
            "\n",
            "--- EPOCH 5/5 ---\n",
            "Batch [1–32] | Loss: 0.009973 | Accuracy: 0.9961\n",
            "Batch [33–64] | Loss: 0.009337 | Accuracy: 0.9964\n",
            "Batch [65–96] | Loss: 0.011332 | Accuracy: 0.9955\n",
            "Batch [97–128] | Loss: 0.011231 | Accuracy: 0.9958\n",
            "Batch [129–160] | Loss: 0.012887 | Accuracy: 0.9951\n",
            "Batch [161–186] | Loss: 0.009745 | Accuracy: 0.9962\n",
            "\n",
            "Epoch 5 Summary:\n",
            "➤ Mean Training Loss: 0.010751\n",
            "➤ Mean Training Accuracy: 0.9958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(ground_truth_masksv))\n",
        "print(ground_truth_masksv.keys() if isinstance(ground_truth_masksv, dict) else len(ground_truth_masksv))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWM_mDjcVtB_",
        "outputId": "819cc25b-db81-431b-976d-a175a869205e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set the paths to your test images and labels\n",
        "test_image_dir = \"/content/drive/MyDrive/canal_project/27_11_2025/test/images\"  # <-- Update this path\n",
        "test_label_dir = \"/content/drive/MyDrive/canal_project/27_11_2025/test/masks\"   # <-- Update this path\n",
        "\n",
        "# === Load and Sort Test Image Paths ===\n",
        "# Collect all test image files (e.g., .jpg)\n",
        "all_test_image_paths = sorted(glob(os.path.join(test_image_dir, \"*.jpg\")))\n",
        "test_total_images = len(all_test_image_paths)\n",
        "print(f\"Total Number of Test Images: {test_total_images}\")\n",
        "\n",
        "# === Load and Sort Test Label Paths ===\n",
        "# Collect all test label files (e.g., .png masks)\n",
        "all_test_label_paths = sorted(glob(os.path.join(test_label_dir, \"*.png\")))\n",
        "test_total_labels = len(all_test_label_paths)\n",
        "print(f\"Total Number of Test Labels: {test_total_labels}\")\n",
        "\n",
        "# === Match Image and Label Paths ===\n",
        "# These lists can now be used for DataLoader or evaluation\n",
        "Test_image_paths = all_test_image_paths[:test_total_images]\n",
        "Test_label_paths = all_test_label_paths[:test_total_labels]\n",
        "\n",
        "# Optional: Print a few samples to verify\n",
        "print(\"Sample test image path:\", Test_image_paths[0] if Test_image_paths else \"No images found\")\n",
        "print(\"Sample test label path:\", Test_label_paths[0] if Test_label_paths else \"No labels found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyK20p1xYkUP",
        "outputId": "a828bc83-2df3-4be5-d67d-dd9f38c4b764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Number of Test Images: 40\n",
            "Total Number of Test Labels: 40\n",
            "Sample test image path: /content/drive/MyDrive/canal_project/27_11_2025/test/images/image_capture_2025-04-06_12-11-49_jpg.rf.826be63e91371c207d97f6472d9d495b.jpg\n",
            "Sample test label path: /content/drive/MyDrive/canal_project/27_11_2025/test/masks/image_capture_2025-04-06_12-11-49_jpg.rf.826be63e91371c207d97f6472d9d495b_mask.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dictionary to hold ground truth binary masks for test data\n",
        "ground_truth_test_masks = {}\n",
        "\n",
        "# === Load and Process Each Test Mask ===\n",
        "for idx in range(len(Test_label_paths)):\n",
        "    # Read label image in color (3-channel); expected mask is in the red channel\n",
        "    gt_color = cv2.imread(Test_label_paths[idx])\n",
        "\n",
        "    # Extract the red channel only and convert to binary mask\n",
        "    # Note: OpenCV loads in BGR, so red is at index 2\n",
        "    binary_mask = (gt_color[:, :, 2] > 0).astype(np.float32)\n",
        "\n",
        "    # Resize if specified\n",
        "    if desired_size is not None:\n",
        "        binary_mask = cv2.resize(binary_mask, desired_size, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    # Store in dictionary\n",
        "    ground_truth_test_masks[idx] = binary_mask\n",
        "\n",
        "print(f\"Loaded {len(ground_truth_test_masks)} ground truth test masks.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jApmMGDoYwP-",
        "outputId": "d40e59fe-cf50-4977-cdab-c8c1f4a3b45f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 40 ground truth test masks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# === Inference with SAM Predictor on Test Set ===\n",
        "masks_tuned_list = {}   # Stores predicted binary masks\n",
        "images_tuned_list = {}  # Stores input images used during inference\n",
        "\n",
        "for idx in range(len(Test_image_paths)):\n",
        "    # === Load and Preprocess Image ===\n",
        "    image = cv2.imread(Test_image_paths[idx])\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    if desired_size is not None:\n",
        "        image_rgb = cv2.resize(image_rgb, desired_size, interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    # === Set the image for SAM predictor ===\n",
        "    predictor_tuned.set_image(image_rgb)\n",
        "\n",
        "    # === Predict segmentation mask ===\n",
        "    masks_tuned, _, _ = predictor_tuned.predict(\n",
        "        point_coords=None,\n",
        "        box=None,\n",
        "        multimask_output=False,  # Only get the most confident mask\n",
        "    )\n",
        "\n",
        "    # === Extract and post-process the first predicted mask ===\n",
        "    mask_np = masks_tuned[0, :, :]                 # Select first mask\n",
        "    binary_mask = (mask_np > 0).astype(np.float32) # Convert to float binary mask\n",
        "\n",
        "    # === Store results ===\n",
        "    images_tuned_list[idx] = image_rgb\n",
        "    masks_tuned_list[idx] = binary_mask\n",
        "\n",
        "print(f\"Inference complete on {len(Test_image_paths)} test images.\")\n"
      ],
      "metadata": {
        "id": "GVLcyy0OY2lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# === Grid Configuration ===\n",
        "n_images = len(images_tuned_list)\n",
        "n_cols = 4  # Number of images per row\n",
        "n_rows = (n_images // n_cols) + (n_images % n_cols > 0)  # Auto-calculate rows\n",
        "\n",
        "# Create a figure with subplots\n",
        "fig, axs = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))\n",
        "\n",
        "# If axs is 1D (e.g., only 1 row), convert to 2D for consistency\n",
        "axs = np.atleast_2d(axs)\n",
        "\n",
        "# === Iterate and Plot ===\n",
        "for i in range(n_rows):\n",
        "    for j in range(n_cols):\n",
        "        index = i * n_cols + j\n",
        "        ax = axs[i, j]\n",
        "\n",
        "        if index < n_images:\n",
        "            # Display the RGB image\n",
        "            ax.imshow(images_tuned_list[index], interpolation='none')\n",
        "\n",
        "            # Generate a blue mask overlay (R=0, G=0, B=1) for binary mask = 1\n",
        "            mask = masks_tuned_list[index]\n",
        "            blue_mask_rgb = np.zeros((*mask.shape, 3), dtype=np.float32)\n",
        "            blue_mask_rgb[..., 2] = mask  # Blue channel\n",
        "\n",
        "            # Overlay the mask with transparency\n",
        "            ax.imshow(blue_mask_rgb, alpha=0.5)\n",
        "\n",
        "        # Remove axes ticks\n",
        "        ax.axis('off')\n",
        "\n",
        "# === Final Layout ===\n",
        "plt.subplots_adjust(wspace=0.03, hspace=0.03)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1N-yJbKOY-QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import auc, roc_curve\n",
        "\n",
        "# === Binary Metrics for One Prediction ===\n",
        "def binary_segmentation_metrics(predictions, targets):\n",
        "    \"\"\"\n",
        "    Computes binary segmentation metrics for a single predicted mask vs ground truth.\n",
        "    Inputs:\n",
        "        predictions (numpy array): predicted mask, float32, range [0,1] or binary\n",
        "        targets (numpy array): ground truth mask, binary (0 or 1)\n",
        "    Returns:\n",
        "        Tuple of metrics: accuracy, precision, recall, F1-score, IoU, kappa, FP, FN, TP, TN, dice\n",
        "    \"\"\"\n",
        "    # Flatten and convert to binary\n",
        "    predictions = predictions.squeeze()\n",
        "    targets = targets.squeeze()\n",
        "\n",
        "    predictions_binary = (predictions > 0.5).astype(int)\n",
        "    targets_binary = targets.astype(int)\n",
        "\n",
        "    # Confusion matrix components\n",
        "    TP = np.sum((predictions_binary == 1) & (targets_binary == 1))\n",
        "    FP = np.sum((predictions_binary == 1) & (targets_binary == 0))\n",
        "    FN = np.sum((predictions_binary == 0) & (targets_binary == 1))\n",
        "    TN = np.sum((predictions_binary == 0) & (targets_binary == 0))\n",
        "\n",
        "    # Metrics with small epsilon to avoid division by zero\n",
        "    eps = 1e-5\n",
        "    accuracy = (TP + TN + eps) / (TP + FP + FN + TN + eps)\n",
        "    precision = (TP + eps) / (TP + FP + eps)\n",
        "    recall = (TP + eps) / (TP + FN + eps)\n",
        "    f_score = 2 * (precision * recall) / (precision + recall + eps)\n",
        "    dice = (2 * TP + eps) / (2 * TP + FP + FN + eps)\n",
        "    iou = (TP + eps) / (TP + FP + FN + eps)\n",
        "\n",
        "    # Cohen’s kappa\n",
        "    total = TP + FP + FN + TN\n",
        "    p_o = (TP + TN) / total\n",
        "    p_e = ((TP + FP) * (TP + FN) + (FN + TN) * (FP + TN)) / (total ** 2)\n",
        "    kappa = (p_o - p_e) / (1 - p_e + eps)\n",
        "\n",
        "    return accuracy, precision, recall, f_score, iou, kappa, FP, FN, TP, TN, dice\n",
        "\n",
        "# === Average Metrics Across Dataset ===\n",
        "def calculate_average_metrics(predictions_list, targets_list):\n",
        "    \"\"\"\n",
        "    Computes average binary segmentation metrics across a dataset.\n",
        "    Inputs:\n",
        "        predictions_list: dictionary or list of predicted masks\n",
        "        targets_list: dictionary or list of ground truth masks\n",
        "    Returns:\n",
        "        Dictionary of averaged metrics\n",
        "    \"\"\"\n",
        "    num_masks = len(predictions_list)\n",
        "\n",
        "    total_metrics = {\n",
        "        'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f_score': 0.0,\n",
        "        'iou': 0.0, 'kappa': 0.0, 'FP': 0, 'FN': 0, 'MAR': 0.0, 'FAR': 0.0, 'dice': 0.0\n",
        "    }\n",
        "\n",
        "    for i in range(num_masks):\n",
        "        pred = predictions_list[i]\n",
        "        gt = targets_list[i]\n",
        "        metrics = binary_segmentation_metrics(pred, gt)\n",
        "\n",
        "        # Accumulate each metric\n",
        "        for metric_name, value in zip(total_metrics.keys(), metrics):\n",
        "            total_metrics[metric_name] += value\n",
        "\n",
        "        # Add False Negative Rate (Missed Alarm Rate, MAR) and False Alarm Rate (FAR)\n",
        "        TP, TN, FP, FN = metrics[8], metrics[9], metrics[6], metrics[7]\n",
        "        total_metrics['MAR'] += FN / (FN + TP + 1e-5)\n",
        "        total_metrics['FAR'] += FP / (FP + TN + 1e-5)\n",
        "\n",
        "    # Compute mean for each metric\n",
        "    avg_metrics = {k: v / num_masks for k, v in total_metrics.items()}\n",
        "\n",
        "    return avg_metrics\n",
        "\n",
        "# === Example Usage ===\n",
        "# Evaluate the SAM predictions vs. ground truth test masks\n",
        "avg_metrics = calculate_average_metrics(masks_tuned_list, ground_truth_test_masks)\n",
        "\n",
        "# Print results\n",
        "print(\"\\n=== Average Metrics on Test Set ===\")\n",
        "for metric_name, value in avg_metrics.items():\n",
        "    print(f\"{metric_name.upper():<8}: {value:.4f}\")"
      ],
      "metadata": {
        "id": "xmATY1JsY_1T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}